Submission,Status,Name1,Name2,Email1,Institution,Title,Field,Topics,Abstract,Desired,Comments1,Special,Other,Comments2
strack103s1,Accept,Erik,Boman,egboman@sandia.gov,Sandia National Laboratories,Novel preconditioners to speed up linear solvers,"Applied mathematics, computer science","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","The most expensive part of many simulations is the linear solves. Preconditioners are critical to accelerate iterative solvers for large, sparse problems. We will develop and implement algebraic preconditioners that can be used as a black-box for a wide variety of matrices. We propose a new variation of incomplete factorizations. An initial implementation may be done in Matlab or Python, but the eventual goal is to do a parallel implementation and study its performance. If good results are obtained, a research publication is a strong possibility. The software may potentially become part of the Trilinos framework, and thus accessible to thousands of researchers world-wide.","Numerical linear algebra, computer programming, familiar with version control (e.g., git/github), some experience with GPU and/or parallel programming desired.",,Minimum GPA (specify what GPA in comments below); In-Person Only; Permanent Resident OK,,3.0 GPA minimum
strack104s1,Accept,Ravi,Patel,rgpatel@sandia.gov,Sandia National Laboratories,Uncertainty quantification for operator learning under physics informed constraints,Scientifc Machine Learning,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); Machine Learning and AI","One promising strategy in scientific machine learning seeks to obtain a PDE description of a physical system by inferring the PDEâ€™s operators directly from observations of the system. Ideally, a modeler would also enforce a priori known physics constraints such as conservation and symmetries to guarantee the model is at least physically valid. However, observational data is always noisy and sparse, so learned operators are only useful insofar as their uncertainties have been properly quantified. Without these measures, an analyst using the models cannot determine in which regimes the model is valid. The staff members have previously developed methods for learning uncertainty aware operators. The intern will focus on combining these operators with physics informed constraints and studying the interaction. While the methods developed will be broadly applicable, we will focus on two exemplars, crack propagation of a composite under shear loading and climate forcing under volcanic eruption. This topic will blend together ideas from a variety of subjects, e.g., PDEs, statistics, machine learning, solid mechanics, and climatology.","(Not necessarily required) familiarity with coding, machine learning, Bayesian inference, PDEs, and HPC",,Permanent Resident OK; International OK,,
strack105s1,Accept,Stefan,Wild,wild@lbl.gov,Lawrence Berkeley National Laboratory,Learning while optimizing,Numerical optimization and learning,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); Machine Learning and AI",Our research addresses mathematical optimization of challenging computational science problems. We will be exploring ways that we can use machine learning on the data generated internally by an optimization algorithm in order to improve its performance on a chosen application.,"Desire to collaborate, basic algorithm knowledge and experience in a programming language.",,International OK,,
strack106s1,Accept,Sri Hari Krishna,Narayanan,snarayan@mcs.anl.gov,Argonne National Laboratory,AI-Based Adjoints to Diagnose the Sensitivity of Ocean Models to Parameters,"AI, Machine Learning, Earth Sciences","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing; Machine Learning and AI","We are interested in understanding the sensitivities of an ocean model's output, to the model parameters. Estimating this sensitivity is very time consuming by brute force. Alternatively, adjoints have shown great promise in uncovering sensitivity of the model to its parameters. Yet adjoints are very time consuming to develop manually and very involved to develop via automatic differentiation (AD) for some models. Because neural networks implemented in machine learning frameworks can be differentiated trivially, we have want to explore how to generate an accurate neural network (NN) surrogate for an ocean model. We then use our NN model to generate adjoint versions of the original model.","Experience in ML frameworks, models.",The project can be tailored to individual interests and expertise,Permanent Resident OK; International OK,,
strack107s1,Pre-Match+Workshop,Andy,Nonaka,AJNonaka@lbl.gov,Lawrence Berkeley National Laboratory,Multiphysics and Multiscale Modeling with AMReX,Modeling and Simulation,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing; Machine Learning and AI","The Center for Computational Sciences (CCSE) and Engineering at LBL is home to applied mathematicians, computer scientists, and domain scientists that perform modeling and simulation of complex PDE systems across a variety applications. We are seeking collaboration with scientists interested in working on existing CCSE projects and/or developing their models and algorithms using the AMReX framework, an exascale ready infrastructure that supports structured grid, particle/particle-mesh, and machine learning algorithms. Active applications in CCSE include astrophysics, cosmology, multiphase flow, accelerators, plasma physics, microelectronics, atmospheric flows, oceanic flows, microscale flow, and stochastic mesoscale systems.","Numerical analysis, numerical ODEs and PDEs, C++, python, parallel programming, GPU programming, domain science expertise.",,In-Person Only; Permanent Resident OK; International OK,,
strack108s1,Accept,Lingda,Li,lli@bnl.gov,Brookhaven National Laboratory,Building a Program Database to Facilitate Machine Learning-based Computing Research,Computer Science,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Machine Learning and AI","Machine learning (ML) has shown great success in many domains, and increasing efforts are applying ML in the field of computer science and engineering, for instance, to design more efficient hardware and software. Due to its data driven nature, it requires thousands if not millions of programs to generate training data in ML techniques, and the computing research community does not have such a collection of programs in hand. Particularly, traditional benchmarks are short in number, and source code hosting websites (e.g., GitHub) have abundant code snippets instead of executable programs. This project aims to bridge this gap by creating a database including a large number of executable programs. The main task of this project is to collect/write programs that stretch both computing power and memory capability of modern computers, leveraging coding exercise websites such as LeetCode and CodeForces. This database will subsequently be used for program performance predictive model training and testing. Besides the main purpose mentioned above, this project will also help students sharp their programming skills and benefit their future careers in both academia and industry, giving the fact that these coding exercise websites are the de facto means to prepare technical interviews.","Major in computer science/engineering or related fields; proficiency at one or more compilable programming languages (e.g., C/C++); knowledgeable or interest to learn about computer hardware/architecture.",,Permanent Resident OK; International OK,,
strack110s1,Accept,Benjamin M.,Feinberg,bfeinbe@sandia.gov,Sandia National Laboratories,Software and System Architectures for Analog Linear Algebra Accelerators,Computer Systems,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Machine Learning and AI; National Security","Analog computing has significant potential to enable new applications on autonomous systems by providing order-of-magnitude improvements in computational efficiency. Unfortunately, most of the work on analog hardware has focused on device and circuit optimization rather than flexible and programmable systems. To remedy this issue, this project will develop components of a system software toolchain including tools to effectively map applications to the target hardware and manage other runtime components. Additionally, this project will use Sandia's Structural Simulation toolkit to evaluate the developed software on a simulated system and evaluate potential architectural optimizations. As part of this evaluation, we will consider trade offs between programmability and system-level performance and efficiency for a variety of applications. This project will involve interdisciplinary collaboration with both application and circuit/device experts to ensure that the developed toolchain is useful for application developers and provide feedback on architectural challenges that could be mitigated at the circuit or device level.","-Experience with C/C++ -Basic knowledge of computer architecture, compilers, or programming models -Interest in novel computing concepts",The project described above has numerous individual components which can be tailored to specific skills and interests of the SRP participant. I do not expect an SRP participant have knowledge or interest in all aspects of this project.,Minimum GPA (specify what GPA in comments below); other,Neither US Citizen or In-Person are required but both are strongly preferred,Center Requires a 3.0 GPA
strack111s1,Accept,Emil,Constantinescu,emconsta@anl.gov,Argonne National Laboratory,Physics-based machine learning and uncertainty quantification,"ML, PDEs, numerical simulation, uncertainty quantification",Machine Learning and AI,"This project aims to advance computational science by integrating numerical methods for solving partial differential equations (PDEs) with machine learning and statistics for uncertainty quantification. Traditional simulation models have limitations in capturing complex behaviors inherent in systems like weather forecasting, climate modeling, power grid management, and nuclear physics. By incorporating machine learning algorithms into the numerical solving of PDEs, we aspire to build more accurate and efficient simulation models. Additionally, we will employ statistical methods for uncertainty quantification to provide more reliable predictions and to understand the range of possible outcomes. The ultimate goal is to develop a unified computational framework that is versatile enough for applications in various disciplines, from predicting severe weather events to optimizing power grid operations and advancing our understanding of nuclear physics phenomena. We want to explore new approaches to modeling complex systems, offering more accurate and reliable predictions for scientific inquiry and practical applications.","Machine Learning, Python, Numerical simulation of PDEs, uncertainty quantification",,In-Person Only,,
strack112s1,Pre-Match+Workshop,Steven,Hofmeyr,shofmeyr@lbl.gov,Lawrence Berkeley National Laboratory,Extreme-scale metagenomics,"HPC, bioinformatics","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing; Machine Learning and AI","I am looking for students interested in working on metagenomics, especially large-scale metagenome assembly, and machine learning for metagenomics. Here at LBL we have consistently done the largest ever metagenome assemblies, and are working on improving our code base, as well as researching new algorithms and approaches, such as with AI/ML.","Familiarity with HPC systems, competence with C++ and python. Bioinformatics and AI/ML knowledge is a plus",,Permanent Resident OK; International OK,,
strack113s1,Pre-Match+Workshop,Jose M.,Monsalve Diaz,jmonsalvediaz@anl.gov,Argonne National Laboratory,"A learning experience of compilers, runtime systems and parallel programmin","Compilers, Runtimes, Parallel programming, OpenMP, LLVM, HPC","Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software)","High-Performance Computing is an exciting field that constantly pushes the limits of computation. HPC is a complex field. Not only is it necessary to have scientific knowledge of the applications, but also to have a deep understanding of how systems work. The more knowledge of the systems and tools, the higher the scientific impact could be. In this project, we seek to allow learning about compilers, runtime systems, and parallel programming. We aim to find a project that considers the needs of the current projects at ANL in addition to the professional development of the participants. We plan on using parallel programming models such as OpenMP, SYCL, OpenACC, and others. We explore innovation in their runtime systems and compilers. We will use the LLVM compiler infrastructure as one of the most important compilers in the area of HPC. By the end, we would like to encourage participants to 1) be part of the LLVM community, 2) better understand compilers, runtime systems, and parallel programming, 3) get involved in the area of HPC, and 4) create the necessary grounds for professional development.","The following is a list of topics we could work on. It is not a checklist of requirements, but a list of topics we can use to start the discussion. - Programming languages:  - C, C++ or Fortran  - Python - Parallel programming:  - Threading libraries  - OpenMP, OpenACC, SYCL, or similar  - MPI, RPC, or similar - Understanding of parallel programming concepts - Understanding of compilers:  - User-level understanding (e.g., compiler flags)  - CS level understanding (e.g., compilation pipeline and compiler stages)  - Developer-level understanding (e.g., LLVM development) - HPC:  - Cluster infrastructure  - HPC network interconnects  - HPC applications",,International OK,,
strack114s1,Accept,Destinee,Morrow,dmorrow@lbl.gov,Lawrence Berkeley National Laboratory,Using Machine Learning for Early Detection of Obstructive Sleep Apnea,AI in Healthcare,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","Obstructive sleep apnea (OSA) affects 24% of all Veterans or 1 in every 15 Americans and is associated with increased risk for developing cardiovascular and metabolic comorbidities such as heart disease, stroke, hypertension, and type 2 diabetes mellitus. A vast number of patients are already experiencing at least one of these comorbidities by the time they are diagnosed with OSA; suggesting that OSA is being diagnosed well after the onset. OSA shares many symptoms with other diagnoses such as depression, increasing the diagnosis gap. With the use of machine learning and natural language processing, we can better understand and identify OSA and target patients for treatment closer to the onset, reducing the likelihood of patients developing comorbidities.","python, pytorch, mpi, dask, R, clinical data, big data, electronic health records, high-performance computing, natural language processing, large language models, machine learning, predictive modeling",Remote work only.,other,Remote work only,
strack116s1,Accept,Oksana,Guba,onguba@sandia.gov,Sandia National Laboratories,Improving vertical discretizations in atmospheric modeling,"Numerical methods, atmospheric modeling","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing","Due to the specifics of the flow in atmospheric global circulation models, horizontal and vertical dimensions are discretised very differently. The atmospheric dynamical core in the climate model that we use is formulated for hybrid pressure-based vertical levels. There are a few research questions that are related to the vertical discretizations in our model. For example, while having more vertical levels is better for model's fidelity, their number is restricted by related computational cost. Therefore there is a question of how and where to place a limited number of vertical levels while increasing the model's fidelity and usability for applications. The project can be focused on a few relevant aspects, including developing numerical methods, evaluation of the model for climate applications, or uncertainty quantification.","The project would be tailored to the participant's interests and background, assuming it is relatively connected to the atmospheric modeling.",,Minimum GPA (specify what GPA in comments below); In-Person Only; Permanent Resident OK,,Min GPA 3.0
strack118s1,Accept,Drew,Paine,pained@lbl.gov,Lawrence Berkeley National Laboratory,Understanding Scientific User Needs,"User Experience (UX), Computer Supported Cooperative Work (CSCW), Human Computer Interaction (HCI)","Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing","User Experience (UX) research &amp; design is the practice of investigating the real world needs of humans and designing solutions to improve their ability to use computational systems. UX work at LBNL investigates the needs of scientific users (e.g., Jupyter users at NERSC, scientists at the ALS) and designs software to enable them to more efficiently and effectively do their research. Conducting UX work provides teams with the opportunity to diversify their points of view and dig into the challenges their focal users face on a daily basis, building empathy and reframing the software production process. UX also impacts the sustainability of software by improving the likelihood that it will develop a user base who are vital to its continued existence.",qualitative research experience is ideal research question / hypothesis development,,other,virtual is great,I am remote and will be virtual only.
strack119s1,Pre-Match+Workshop,Doru Thom,Popovici,dtpopovici@lbl.gov,Lawrence Berkeley National Laboratory,Hardware Software Co-Design,Computer Science,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","The hardware landscape is constantly changing to match the needs of scientific applications. CPUs have provided much of the compute power, however recently the focus has shifted towards highly parallel GPUs and specialized units customized for specific operations like matrix multiply or Fourier transforms. As such, library and framework developers need to be always active to adapt the software but also keen on offering information to hardware developers for better hardware support. Moving forward, more emphasis should be put on hardware-software co-design, namely developing strategies to optimize certain algorithms, creating models to guide the optimizations and the hardware itself, and then automating the process by using high level frameworks like JAX or others. This becomes even more critical nowadays when computation is applied on data that exhibits dynamic and sparse behavior. This additional dimension makes the search space for efficient algorithms and hardware harder. Therefore, the work I am doing is looking into ways to reduce the search space and offer solutions for various scientific applications in a short amount of time, whether those solutions are all in software or whether some components make use of specialized hardware.","C/C++ background, a little bit of GPU programming, enthusiasm to get hands dirty and benchmark algorithms and systems",,Permanent Resident OK; International OK,,
strack120s1,Accept,Kristofer,Bouchard,kebouchard@lbl.gov,Lawrence Berkeley National Laboratory,Foundation model for proteins,AI and biology,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.)","Foundation models are massive (&gt;1B parameters) deep networks that have been pre-trained on large and diverse data sets and can then be transfer-learned to other relevant tasks. Salient examples include GPT-4, DALL-E2, etc., While this class of models has been impactful in industrial applications, foundation models for science are nascent. Proteins are fundamental units of biological processes. This project will build of diverse extent work across LBL to create a foundation model that integrates all known protein structure and sequences. This protein foundation model could then be used as the basis for, e.g., generating novel protein sequences with enhanced functionality, inferring atomic structure from SAXS data, or inferring the function of newly discovered proteins.","pytorch, large-language models, transformers, graph neural networks,",,other,none,
strack122s1,Accept,Massimiliano,Lupo Pasini,lupopasinim@ornl.gov,Oak Ridge National Laboratory,Surrogate models for materials science,Artificial Intelligence for Materials Science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","Material design and discovery are crucial for the US Department of Energy (DOE) in order to advance energy technologies, improving efficiency, and addressing sustainability and environmental challenges. Exploring the space characterized by different materials is extremely complex due to the myriad of possibilities to mix different natural elements at different percentages. Such exploration is impractical given traditional technologies. In fact, laboratory experiments are labor-intensive, and physics computational models require massive computational resources. Both experimental and computational approaches preclude an effective (fast and thorough) exploration of several materials. Artificial Intelligence (AI) and Machine Learning (ML) enable an effective exploration of a several materials within a fraction of the time required by traditional experimental and computational methodologies. This internship offers the opportunity to become acquainted with the scientific challenges that the US-DOE is facing to achieve materials design and discovery, and appreciate the advantages that AI and ML can provide to this field.","The applicant should desirably be familiar with basic AI and ML concepts. Familiarity with Python packages for ML (e.g., Scikit-Learn and PyTorch) is desired, but not required.",,Minimum GPA (specify what GPA in comments below); In-Person Only; U.S. Citizen Only; Permanent Resident OK; International OK,,3
strack123s1,Accept,Qiulan,Huang,qhuang@bnl.gov,Brookhaven National Laboratory,Data Popularity and Data placement Optimization for big data Analysis,"Large scale storage systems, storage optimization and data analytics","Data Science (i.e., data analytics, data management &amp; storage systems, visualization); Machine Learning and AI","Scientific experiments and computations, especially those in NP &amp; HEP programs, are generating and accumulating data at an unprecedented rate. Big data provides opportunities for new scientific discoveries. Nevertheless, for the Scientific Data and Computing Center , managing the vast amount of data cost-effectively while enabling efficient data analysis in a large-scale, multi-tiered storage architecture is a real challenge. The topic revolves around the exploration and development of techniques aimed at comprehending data popularity and optimizing data access. Through studying data access patterns, doing data analytics, we can identify frequently accessed datasets, prioritize their availability and subsequently design a policy engine to enhance resource allocation for analytical tasks. This research topic assumes critical importance in today's data-driven world, as it holds the potential to significantly improve data analysis efficiency, enhance decision-making processes, and fuel innovation across a diverse range of domains.","High-performance storage system(dCache, Lustre), big data analytics, AI/prediction modeling, monitoring tools like ELK, Python, C/C++, Java, Linux",,Minimum GPA (specify what GPA in comments below); International OK,,3.5
strack124s1,Accept,George,Michelogiannakis,mixelogj13@yahoo.co.uk,Lawrence Berkeley National Laboratory,"Computer architecture, system profiling, and networking","Computer architecture, networking, data analysis","Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","The umbrella of my research is high performance computing (supercomputers) as well as applications. The task broadly is to design the supercomputers for 10-20 years from now. There are a few potential topics that look for adjacent skills. One possibility is research on computer architecture that uses knowledge of hardware description languages or FPGAs. Another possibility is networking (either on-chip or system-wide) research on new router architectures, flow control and such by the use of a simulator. Another possibility is profiling production systems by the use of perf or similar tools followed by analyzing and creating new metrics and plotting results in order to extract insights. Other related topics are also possible such as programming models, algorithms, and job scheduling. A combination of the above topics is also possible and encouraged. In addition, if faculty or students propose interesting and related topics that will also be great because it can lead to new areas of research.","A subset of the following: Computer architecture, hardware description languages, data analysis with python or similar, profiling systems, network knowledge, use of simulators for networks.","Exploring new topics is encouraged. The aforementioned research topics include machine learning techniques in multiple levels so such an experience is desired. Applying for external funding opportunities is encouraged to make sure that does not become a constraint. Also, unfortunately I'm on travel during the workshop dates. I'll try my best to attend what I can.",Permanent Resident OK; International OK,,No requirements from the above
strack125s1,Accept,Julien,Loiseau,jloiseau@lanl.gov,Los Alamos National Laboratory,Co-Design Summer School 2024,HPC,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","The CDSS 2023 subject will focus on modeling radiation hydrodynamics using the FleCSI framework. This work can be decomposed in two steps: the radiative transfer library and the hydrodynamics code. The hydrodynamics part will feature a mesh based implementation for our test, either Lagrangian or Eulerian. In the radiation library implementation, we will use flux limited diffusion as a starting point. Multiple different approaches can then be explored for better treatment of the radiation energy, such as the multigroup method. This base code will enable us to run interesting simulations including Lowrie radiating shock, Su-Olson Marshak wave, and ablation problems. The complexification of supercomputers, in both number of nodes and on-node hybridization, forces us to rethink our approach to high performance computing. In this context, task-based parallelism provides a promising path forward. The summer school proposes to use the FleCSI framework as a base for the simulations. FleCSI is a compile time configurable framework from LANL which supports the development of multiphysics applications. LANL is funding a next generation exascale supercomputer called Venado. The aim for the summer school is to run the simulations at scale on Venado, taking advantage of the NVIDIA superchip.","Computer Scientists: HPC, Kokkos/CUDA/HIP, MPI. Applied Mathematicians: Iterative and Multigrid Methods. Preconditioners. Physicists: Hydrodynamics, Radiative Transfer (diffusion, multigroup etc), Shock Physics.",,In-Person Only; U.S. Citizen Only; Permanent Resident OK; other,Graduate Students,
strack127s1,Accept,Ajeeta,Khatiwada,ajeeta@lanl.gov,Los Alamos National Laboratory,Application of Machine Learning in Nuclear Data Evaluation,Nuclear Physics,"Data Science (i.e., data analytics, data management &amp; storage systems, visualization); Machine Learning and AI","Nuclear Data libraries, which contain information about the interaction of particles with nuclei, are carefully curated from experimental data and theoretical predictions. This data includes details about nuclear reactions, such as their reaction probability (cross section), decay yields, spectra of the outgoing particles etc. and are used to understand/predict the behavior of particles in nuclear systems, such as nuclear reactors, astrophysical processes, radiography, gamma-based interrogation techniques etc. As such, any inaccuracies and imprecision in the nuclear data gets propagated to the uncertainties in the application of interest. Until recently, most general purpose nuclear data libraries have utilized Bayesian approach to tune the theory model input parameters to fit the experimental data. In this project, the student(s) will explore machine learning based approach to combine the theoretical models with experimental data to come up with evaluated nuclear data for specific reaction channels and physics observables. Upon the successful completion of this work, the work will be published in peer reviewed journals.",* Hard working * Friendly * Team player * Preference to someone with background in physics and/or math * Familiarity with coding (preference to Python) * Familiarity with/interest in Machine Learning algorithms,,Minimum GPA (specify what GPA in comments below); In-Person Only; U.S. Citizen Only,,GPA&gt;3.25
strack128s1,Accept,Alina,Kononov,akonono@sandia.gov,Sandia National Laboratories,Quantum Dynamics Simulations,Computational Physics,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Quantum Computing and Information Science","Quantum dynamical simulations can predict important properties like opacity and conductivity that govern how materials interact with light and electric fields. In addition to materials engineering applications like solar cells, these properties determine the behavior of fusion energy experiments. State-of-the-art simulation techniques for computing these properties are run on supercomputers and still require many physical and numerical approximations that degrade accuracy in favor of feasibility. Someday, quantum computers may allow more efficient and precise calculations, but requisite algorithms are still being developed and analyzed. This project will characterize the numerical behavior of dynamical simulations of model systems and help inform methodological improvements and limitations for both classical and quantum computers, or explore another relevant aspect of mutual interest.","Proficiency with Python (or another programming language) and familiarity with undergraduate-level quantum physics will be helpful. Interest in dynamical systems, quantum computing, materials science, plasma physics, or related areas is desirable.",,Minimum GPA (specify what GPA in comments below); Permanent Resident OK,,3.0 GPA
strack129s1,Accept,Xingfu,Wu,xingfu.wu@anl.gov,Argonne National Laboratory,Autotuning Scientific Applications at Scale,"HPC, ML","Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing; Machine Learning and AI","As we enter the exascale computing era, efficiently utilizing power and optimizing the performance of scientific software under power and energy constraints are challenging. Scientific software developers often rely on HPC systems with the default configurations setup by the vendors to run their applications, however, their applications are not efficiently executed with the default system configurations. The number of tunable parameters that HPC users can configure at the system and software levels has increased significantly because of the complexity of the HPC ecosystems, resulting in a dramatically increased parameter space. Attempting to evaluate many (or all) possible parameter combinations becomes very time-consuming. Therefore, scientific application developers will be able to use our publicly available autotuning software package ytopt (https://github.com/ytopt-team/ytopt) to autotune their applications on the target HPC systems to identify the best configuration (for software and system parameters) and then use the best configuration to run their applications efficiently on the target systems. This approach not only optimizes scientific software for efficient execution and energy efficiency but also saves considerable energy on exascale supercomputers at DOE leadership computing facilities.","Desired relevant skills: python and any programming language (C, or Fortran, C++) Bonus: Some scientific application needs fine-tune on HPC systems",,Minimum GPA (specify what GPA in comments below); In-Person Only,,3.5
strack130s1,Accept,Nathan,Urban,nurban@bnl.gov,Brookhaven National Laboratory,Neural partial differential equations,Applied mathematics,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","Numerical computer simulation models are used to predict the behavior of physical systems such as fluids or materials. These simulations are based on underlying partial differential equations (PDEs). Neural partial differential equations (NPDEs) are a machine learning (ML) approach that replaces these physical governing equations with a neural network. This ML approach is used when the equations describing a systemâ€™s behavior are not fully understood from a physical perspective, because it permits (1) learning unknown governing equations from data, and (2) quantifying uncertainties in the mathematical form of learned equations. ""Hybrid"" simulations are also possible, with some of the equations specified from physical principles and others learned from data as neural networks. In this project, we will explore the potential for NPDEs to describe various physical systems, such as the heat diffusion equation, or more complex systems like the reaction-diffusion equations describing spatially-distributed chemical reactions, or the Cahn-Hilliard equations describing phase separation in self-assembling nanomaterials. This can include studying the ability of the neural network to emulate the behavior of the original system, as well as Bayesian statistical and mathematical dimension reduction methods to efficiently quantify uncertainties of the NPDE in a reduced-dimensional space of neural network parameters.","Scientific programming Mathematics (ideally at the level of differential equations, linear algebra, or multivariate calculus)",,In-Person Only; Permanent Resident OK; International OK,,
strack131s1,Accept,Liwen,Wan,wan6@llnl.gov,Lawrence Livermore National Laboratory,Multiscale modeling of electrochemical interfaces,"Materials Science, Chemistry, Chemical Engineering, Mechanical Engineering","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing; Machine Learning and AI","This project aims to understand degradation at the electrolyte-electrode interfaces during cell preparation and electrochemical cycling. We will leverage various computational methods developed at LLNL to address how degradation precursors are formed during initial chemical or electrochemical reactions and how these precursors dynamically evolve to form an inhomogeneous solid electrolyte interphase (SEI). In addition to the understanding of SEI formation, we will elucidate the implication of SEI formation to ion transport kinetics and address the impact of microstructural features of the SEI.","Familiarity with Linux operating system and high-performance computing environment, Fundamental knowledge of general chemistry, thermodynamics, kinetics and computational materials science, Basics of data analysis, Fundamentals of electrochemistry",,Minimum GPA (specify what GPA in comments below); Permanent Resident OK; International OK,,Minimum GPA: 3.5
strack132s1,Accept,Khaled,Ibrahim,kzibrahim@lbl.gov,Lawrence Berkeley National Laboratory,HPC Workflows Performance Modeling and Tuning,"Performance Modeling, tuning, and Optimization","Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing; Machine Learning and AI","In the performance and algorithms group, we tackle various applications and workflow performance optimization problems, using refactoring techniques of these codes to leverage DOE supercomputing machines efficiently. We aim to enable the development of cutting-edge solutions to tackle computationally challenging problems. Our SRP visitors are expected to engage in ongoing research efforts within our group to engage in an experience in developing performance modeling and tuning methods. We also encourage application developers with performance challenges to engage with us to apply our developed methods in improving the performance of their code in leading DOE computational environments. We also encourage application developers with performance challenge to engage with us to apply our developed methods in improving the performance of their code in leading DOE computational environments.","Skills in performance modeling, profiling, and tuning. Familiarity with HPC programming models and/or with DL and ML frameworks.",,In-Person Only,,
strack134s1,Accept,Erik,Palmer,epalmer@lbl.gov,Lawrence Berkeley National Laboratory,Supercomputer User Ticket Modeling and Analysis for Improving Scientific Output,Data Science/Applied Math/HPC,"Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","At the National Energy Research Scientific Computing Center (NERSC), we cater to a user base exceeding 9,000 individuals, facilitating groundbreaking scientific endeavors on the 8th fastest supercomputer globally. As part of our support team, our primary mission is to assist scientists and researchers in using this powerful resource for a variety of scientific computation. Our responsibilities encompass troubleshooting application issues, optimizing performance, and maintaining the computing hardware and software environment. In order to do this, we would like to make data-informed decisions based on user behavior by analyzing support ticket content to identify and address user needs strategically. For this project, we would like to use our ticket text information to gain insight into user behavior, sentiments and needs regarding high performance computing. We are open to working with a variety of tools and techniques to meet this goal. This could include machine learning driven analysis techniques such as natural language processing and data clustering powered by our own supercomputer. Or it could involve simple statistical analysis techniques as these models can also be effective tools for aiding understanding. A successful project will produce data artifacts that help identify user trends or behaviors to guide decisions about user services.","We would like students who value listening skills, exhibit perseverance by working alone or reaching out for help from others. Students who regularly and proactively check-in for assistance and progress updates are also highly desired. Knowledge of coding, statistics, machine learning, and language processing are useful, but not required as mentees will be given time, resources and our guidance to learn. The project mentors would like to use Python as the primary language, so familiarity with Python is ideal. We believe there is a lot of room to move in different directions with this project depending on your own interests.",,Permanent Resident OK; International OK,,I believe we can accommodate most US-based students.
strack135s1,Accept,Kathryn,Maupin,kmaupin@sandia.gov,Sandia National Laboratories,Enhancing model productivity through model form error quantification,"Computational modeling, uncertainty quantification","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Machine Learning and AI","Despite continuing advances in statistical inversion and modeling, model inadequacy due to model form error remains a concern in all areas of mathematical modeling. The Bayesian paradigm naturally integrates uncertainties from both experimental data and model formulation, including initial or boundary conditions, model form, and parameter and numerical approximation. While model improvement is an enterprise that is continuously enabled by the availability of cost-effective high-performance computing infrastructure, model error is unavoidable in many situations. This problem is attributed to the incomplete understanding of the underlying physics, likely in addition to large and poorly characterized uncertainties in calibration and validation data. Introducing a model discrepancy term into the Bayesian framework can improve the predictive power of a given model and, arguably, the transferability of physical parameters. Much like physical models, calibrating a discrepancy model requires careful consideration regarding formulation, parameter estimation, and uncertainty quantification, each of which is often problem-specific. This project seeks to explore methods for model form error quantification that can be easily generalized for wide applicability. Of particular interest are methods that may enable physics discovery and enhance extrapolation.","Desired interests: Surrogate modeling and computational modeling Desired skills: Proficiency in at least one programming language (preferably C, R, or python) Relevant skills: Machine learning or surrogate/reduced order modeling methods",,other,N/A,
strack137s1,Accept,Sarah,Poon,sspoon@lbl.gov,Lawrence Berkeley National Laboratory,User Interfaces for BRaVE: National Biopreparedness and Response Capabilities,User experience (UX) design and research and frontend engineering for science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization)","The BRaVE initiative, which is developing countermeasure against emerging biological treats, needs user interfaces and data dashboards to enable scientists developing diagnostics, vaccines, and therapeutics, to make sense of their data and make decisions for which pathways to pursue. This project will focus on trying to make scientific data accessible, understandable, and actionable, as part of the effort to be prepared for the next pandemic.","Interest in: web development (javascript, html/css, etc), user experience design and research, information visualization, data dashboards",,Permanent Resident OK; International OK,,
strack138s1,Accept,Davis,Herring,herring@lanl.gov,Los Alamos National Laboratory,FleCSI Task Generalization,parallel library programming,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","Task-based parallelism imposes constraints on units of computational work to allow them to be dynamically scheduled for optimum hardware utilization. More sophsticated models for launching tasks can relax certain of these constraints and afford applications more expressive power in writing their tasks. Furthermore, additional information provided by the application can be used to optimize the execution of tasks with unusual semantics (e.g., that need filesystem access). We plan to explore the space of task semantics, identify which generalizations can be efficiently supported in FleCSI, and extend it to provide those which are most valuable for practical applications.","C++ programming parallel programming (in any or all of MPI, CUDA, Kokkos, OpenMP, or kernel threads) high-performance computing environments computational methods","FleCSI utilizes, directly or indirectly, various sophisticated programming techniques, in parallel or otherwise. We expect that training in these techniques would be part of the collaboration.",International OK,,
strack139s1,Accept,Jan,Ciesko,jciesko@sandia.gov,Sandia National Laboratories,Next Gen Communication APIs in HPC,GPU Parallel Programming,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","Communication offloading to GPUs represents a hot topic in science and industry. While many algorithms could benefit from this capability in terms of performance and programmability, novel communication APIs are required that are more suitable for GPU execution. In this work we examine the current PGAS support for distributed parallel programming in Kokkos and evaluate that implementation in comparison to proposals coming from the MPI Forum as well as hardware manufactures such as NVIDIA. Using micro-benchmarks we showcase performance and programmability differences and conclude this work with recommendations on future communication APIs for scientific computing and HPC.","Parallel Programming, GPU Programming, C++",,In-Person Only; U.S. Citizen Only; Permanent Resident OK; International OK,,
strack140s1,Accept,Ishan,Srivastava,isriva@lbl.gov,Lawrence Berkeley National Laboratory,Multiscale Modeling of Complex Fluids and Multiphase Flows,Computational Fluid Dynamics,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing; Machine Learning and AI","We are developing numerical algorithms and computational models to simulate the dynamics of particulate materials, complex fluids and multiphase flows. The proposed approach involves integrating a particle-scale representation such as the discrete element method (DEM), a coarse-grained representation such as particle-in-cell (PIC), and a continuum-scale PDE representation of these materials using the tools of adaptive mesh and algorithm refinement, and data-driven machine learning methods. The overarching goal will be a multiscale modeling framework that can simulate a wide variety of particulate materials and complex fluids, and is performant on manycore/GPU-based high performance computing (HPC) platforms. Another goal of this project is to conduct large-scale simulations of complex fluids and multiphase flows on HPC platforms in application spaces such as bioreactors and advanced manufacturing.","Basic knowledge of C++ programming and Python scripting. Background in Applied Mathematics, Physics, or Physical Sciences/Engineering. Basic understanding of applied mathematics, computational methods, scientific computing, and fluid dynamics.",,In-Person Only; Permanent Resident OK; International OK,,
strack141s1,Accept,Hong,Zhang,hzhang@mcs.anl.gov,Argonne National Laboratory,Scientific Computing using the PETSc/TAO Library on Exascale Machines,HPC,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing","Robust, efficient, and scalable numerical solvers for simulations based on partial differential equations (PDEs) and networks are at the heart of computational science. PETSc, the Portable, Extensible Toolkit for Scientific Computation (https://petsc.org), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by PDEs, including related functionality for numerical optimization in TAO (Toolkit for Advanced Optimization). This project will focus on advances in PETSc/TAO composable solvers, which provide the core of scalable multiphysics and multiscale applications, including fusion, geosciences, power grids, nuclear energy, and more. Areas of potential work include developing efficient and scalable algorithms for linear, nonlinear, and timestepping solvers; creating example programs to demonstrate functionality and explore performance on extreme-scale architectures; and advancing new capabilities as motivated by the needs of data-driven computing and machine learning. Students are expected to gain hands-on numerical programming experience on state- of-the-art parallel computers. Students will apply the algorithms and techniques learned to a project in either their own field (particularly encouraged) or suggested by the mentor.","Mathematical understanding, computer coding skills, motivation and hard working",,International OK,,
strack142s1,Accept,Helen,He,yhe@lbl.gov,Lawrence Berkeley National Laboratory,OpenMP Common Core with Python,High Performance Computing Programming Model,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","OpenMP is the de facto standard for writing parallel applications for shared memory computers. Born ~25 years ago in 1997, it runs on just about every shared memory platform on the market. Itâ€™s also very complicated with the Language Specification of over 600 pages. Most OpenMP programmers however only use around 21 items from the Language Specification. We call these 21 items the â€œOpenMP Common Coreâ€. By focusing on the common core, we make OpenMP what it was always meant to be: a simple API for parallel application programmers. Most C/C++/Fortran compilers have OpenMP support. The OpenMP Common Core Book published in 2019 by Tim Mattson, Helen He, and Alice Koniges provided example codes mostly in C (plus the Fortran supplement), to illustrate OpenMP Common Core concepts. PyOMP is a new project based on Numba to allow Python programmers to use OpenMP directives in their Python code. The proposed SRP project is to learn OpenMP Common Core first, then install PyOMP on the NERSC system or your own laptop and implement selected OpenMP Common Core example codes in Python.","Python, HPC parallel programming concept, C/C++ or Fortran",,Minimum GPA (specify what GPA in comments below); Permanent Resident OK,,minimum GPA 3.33
strack143s1,Accept,Nan,Ding,nanding@lbl.gov,Lawrence Berkeley National Laboratory,Explore the performance of GPU-initiated communications and CPU-initiated communication on heterogeneous architectures,HPC Communications,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","Effective programming models offer programmers the ability to harness the capabilities of the underlying platform. For decades, the CPU-initiated Message Passing Interface (MPI) has become a de facto standard for communication among processes running on distributed memory systems. As high-performance GPU computing becomes the trend, GPU-initiated communication becomes a viable solution for multi-GPU scaling. However, the lack of deep understanding of GPU-initiated communication performance and its impact on an application's performance becomes a hurdle. As such, the proposed topic is to explore the performance of GPU-initiated communications and CPU-initiated communication by using or creating representative benchmarks on Perlmutter, Frontier, and possibly Aurora. The work may includes writing benchmark using CPU- and GPU-initiated communication.",knows MPI programming Passion about research work,,Minimum GPA (specify what GPA in comments below); In-Person Only; U.S. Citizen Only; Permanent Resident OK; International OK,,
strack144s1,Accept,Russell,Whitesides,whitesides1@llnl.gov,Lawrence Livermore National Laboratory,Future Clean Jet Fuels,Combustion,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","This project will explore impacts of sustainable aviation fuels on performance of combustion in jet engines. We will investigate fuel property effects on combustion metrics such as stability, efficiency, emissions. To capture the chemical kinetic effects of the fuel we will use detailed models and highly resolved grids, necessitating the use of high-performance computing platforms. The study can be focused on combustion physics, computational performance of the simulation software, or some mix of the two.","Skills: engineering or computer science Interests: Energy, climate, combustion",,International OK,,
strack145s1,Accept,Terry,Jones,trj@ornl.gov,Oak Ridge National Laboratory,Memory Tools for High Performance Computing,Computer Science,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","The recent emergence of new memory technologies and multi-tier memory architectures has disrupted the traditional view of memory as a single block of volatile storage with uniform performance. Several options for managing data on heterogeneous memory platforms now exist, but current approaches either rely on inflexible, and often inefficient, hardware-based caching, or they require expert knowledge and source code modification to utilize the different performance and capabilities within each memory tier. We are researching a new software-based framework to collect and apply memory management guidance for applications on heterogeneous memory platforms. The framework, together with new tools, combines a generic runtime API with automated program profiling and analysis to achieve data management that is both efficient and portable across multiple memory architectures.",â€¢ basic computer science skills â€¢ C programming language â€¢ willingness to learn,,Permanent Resident OK,,
strack146s1,Accept,Benjamin,Priest,priest2@llnl.gov,Lawrence Livermore National Laboratory,HPC-scale data science for applications,"high performance computing, scientific computing, statistics, machine learning","Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","Modern Department of Energy science missions from cosmology to biosecurity to climate to computer security to fusion are collecting increasingly enormous datasets whose exploitation requires novel approaches and algorithms. For example, the Vera C. Rubin Observatory (https://www.lsst.org/) will produce 20 trillion bytes of cosmology image data per night for 10 years, which is well beyond the throughput capabilities of conventional astronomy codes. Furthermore, nucleotide, protein, and antibody sequence catalogues continue to grow in both observation size and count, and so require novel solutions for even basic machine learning tasks such as clustering. Our group works on solutions to these and other problems that are scalable, fast, and both theoretically and statistically motivated. This summer project will involve implementing data science codes for deployment on Lawrence Livermore's statoe-of-the-art computers in service to improving the nation's ability to handle one of these critical problems. We expect the project to involve collaborating with several subject matter experts depending on the specifics of the task, which could include physicists, applied mathematicians, computer scientists, and statisticians.","Python and/or C++, high performance computing, statistics and/or graph algorithms, applied mathematics, interest in applications such as space, climate, or network analysis.",,other,Remote Only,
strack147s1,Accept,Michael,Jantz,mrjantz@utk.edu,Oak Ridge National Laboratory,Simplified Interface to Complex Memory,Software Systems,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","In recent years, multiple concurrent trends, including the proliferation of AI and other data-driven analyses, rising CPU core counts, and the relative stagnation of DRAM performance and capacity scaling, have combined to place enormous strain on modern memory systems. At the same time, several new technologies (e.g., high-bandwidth and non-volatile memories), as well as new memory interconnect options (e.g., Compute Express Link), are bringing new capabilities that can potentially address the limitations of conventional memory hardware. As a result, many computing systems are adopting a diverse mix of memory devices and organizations, with the hope that the unique benefits and capabilities of different technologies can be seamlessly combined into a single architecture. The Simplified Interface to Complex Memory (SICM) project aims to create new tools and infrastructure for applications that execute on architectures with complex memory resources. SICM currently includes a low-level interface that allows applications to view and manage heterogeneous resources directly as well as a high-level interface with tools and facilities to manage memory resources automatically, without requiring changes to application software.","We are searching for students that enjoy programming and are particular interested in systems topics, such as operating systems, compilers, and/or memory allocators. Experience with or interest in tools and analyses for monitoring and understanding program behavior is also desired.",,In-Person Only; Permanent Resident OK; International OK,,
strack148s1,Accept,Tadashi,Ogitsu,ogitsu1@llnl.gov,Lawrence Livermore National Laboratory,Quantum dynamics simulation,Materials Science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.)","Our BES funded software center NPNEQ intends to develop the methods and software to simulate time evolution of coupled electron-ion dynamics beyond perturbative approach, which is crucial for describing quantum nature of nonequilibrium dynamics.","High level programing languages such as C++, Fortran, python as well as being familiar with parallelization methods such as MPI.",,Permanent Resident OK; International OK,,
strack149s1,Accept,Pablo,Seleson,selesonpd@ornl.gov,Oak Ridge National Laboratory,Peridynamics Modeling and Simulation for Exascale Fracture Computations,Applied and Computational Mathematics,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing","Computational fracture modeling is an ongoing challenge in computational science and engineering. Peridynamics is a nonlocal reformulation of classical continuum mechanics designed to simulate fractures thus making it an attractive framework for modeling material failure and damage across many application domains. This has been reflected in an exponential growth in the number of peridynamics researchers and publications worldwide over the last two decades. We have developed at Oak Ridge National Laboratory (ORNL) an exascale-capable, GPU-enabled, and performance-portable peridynamics code, CabanaPD (https://github.com/ORNL/CabanaPD). CabanaPD has been tested on the Oak Ridge Leadership Computing Facility (OLCF) leadership-class computing resources, Summit and Frontier supercomputers, and is built on two main libraries: Kokkos and Cabana, both developed through the Exascale Computing Project (ECP). This project will develop mathematical and computational methods to advance the state of the art of peridynamics modeling and simulation, with the end goal of enhancing current features of the peridynamics CabanaPD code.",#NAME?,,Permanent Resident OK; International OK,,
strack150s1,Pre-Match+Workshop,Tanwi,Mallick,tmallick@anl.gov,Argonne National Laboratory,Spatiotemporal modeling using machine learning techniques,"Spatiotemporal modeling, Graph neural network, Clustering",Machine Learning and AI,"Many real-world phenomena, such as traffic flow on road networks, data transfer on the HPC Interconnect Network, load balancing in power grids, and regional rainfall, are spatiotemporal in nature. These complex systems are dynamic, evolving over both time and space. For the aforementioned scientific studies, it is critical to accurately predict the future behaviors of these spatiotemporal systems, cluster the data into meaningful groups to unveil different patterns and anomalies and accelerate spatiotemporal models by optimizing their performance and reducing computation time. In this project, we aim to develop and fine-tune a machine learning model for enhanced spatiotemporal modeling, targeting real-world phenomena like traffic flow, HPC Interconnect Network traffic, and regional rainfall.","Python programming, TensorFlow, or PyTorch programming",,other,None,
strack151s1,Accept,Jeffrey,Larson,jmlarson@anl.gov,Argonne National Laboratory,Numerical optimization of simulations,Computational Science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing",We are seeking SRP collaborators looking to solve complex numerical optimization problems. Researchers with interests in specific application problems or interests in numerical optimization algorithms are both encouraged.,Researchers at any point in their career looking to apply/develop state-of-the-art numerical optimization techniques to application problems from any scientific domain are desired.,,Permanent Resident OK; International OK,,
strack152s1,Accept,Mah Kadidia,Konate,kadidiakonate@lbl.gov,Lawrence Berkeley National Laboratory,Machine Learning for Jobs queue Wait time prediction,Machine learning,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","The era of supercomputing and data science presents novel challenges and opportunities. Supercomputers such as Cori and Perlmutter, with their incredible processing power, are often tasked with running numerous complex jobs concurrently. Efficient management of these jobs, especially determining their queue wait times, is critical to optimizing the system's overall performance. This project, titled ""Queue Wait Time Prediction of Perlmutter, aimed to address this very issue.","Experience with writing Python code, Experience with machine learning and data visulization",,Permanent Resident OK; International OK,,
strack153s1,Pre-Match+Workshop,Dan,Martin,dfmartin@lbl.gov,Lawrence Berkeley National Laboratory,Tokamak modeling,Numerical modeling,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.)","When finally harnessed, nuclear fusion holds the promise of almost limitless energy. We are partners in a project to better understand turbulence in tokamaks, a form of fusion reactor by improving existing models. Project ideas include improving numerics and performance of the model, adding additional physics to the model, and simulation-based projects to understand specific scenarios.","Specific background depends on the specific project focus, but generally: applied mathematics, software development (C++, python), physics, climate science, etc.",This is a placeholder for a range of project ideas under a range of applications. We can come up with specific projects based on your skills and interests.,Permanent Resident OK; International OK,,
strack154s1,Accept,Otto,Venezuela,venezuela1@llnl.gov,Lawrence Livermore National Laboratory,HPC Cloud DevOps,HPC ML / AI DevOps,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","LLNL and Livermore Computing (LC) invite you to apply to our DevOps internship program next summer. Come and join us as we start building new workflow pipelines, infrastructure for ML models, and intelligent apps. We will explore MLOps which will help Data Scientist in LLNL to automate model training, store, and deploy them at scale. We will be using on-prem Kubernetes/OpenShift cluster with GPU enabled server to build the next DevOps infrastructure for our scientist community.",eager to learn new things! unix / linux,,U.S. Citizen Only,,
strack155s1,Pre-Match+Workshop,Pedro,Valero-Lara,valerolarap@ornl.gov,Oak Ridge National Laboratory,Challenges/Opportunities for the Extreme Heterogeneity and HPC-AI era,High Performance Computing,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","This project is aimed at the implementation, evaluation and optimization of novel performance portable and heterogeneous programming solutions for the upcoming extreme heterogeneity era in computing. The project includes the design of novel software solutions on the available software and hardware platforms. Learning objectives for the applicant include: i) develop HPC codes based on performance portable programming models, such as OpenMP/OpenACC, C++, Kokkos, Julia, task-based runtimes, on current HPC and heterogeneous (CPU+GPU) architectures, ii) acquire skills in both, software solutions and HPC codes implementation, iii) gain experience in performance analysis on HPC architectures.",Computer Science Programming Languages Linux,,In-Person Only; Permanent Resident OK,,
strack156s1,Accept,M. Scot,Swan,mswan@sandia.gov,Sandia National Laboratories,Data Analysis to Speed Up Supercomputers,"Data Analysis, Computer Science, High Performance Computing","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing","Our work would fall under the HPC/Application Metrology umbrella (""metrology"" is the study of measuring stuff). So we want to measure the performance of the supercomputer, the performance of the applications that run on them, and how the two interact. I am interested in finding someone to help me speed up our High Performance Computing (HPC) supercomputers. We know that there are bottlenecks in our simulations, like memory, CPU, network, or filesystem limitations. Often these bottlenecks are temporary, so they are hard to find or reproduce. We need someone to run applications on the supercomputers and then analyze the performance data to find performance anomalies and then try to find a cause for them. When we have data showing problems, we can pass that to the system administrators for them to address, speeding up the supercomputer so more simulations can be done faster.",Interest in learning how to use Linux and the Command Line. Interest in learning how to write Python code to analyze data. Interest in running small-to-large simulations on supercomputers. Interest in working with system administrators and computational physicists.,,Minimum GPA (specify what GPA in comments below); In-Person Only; U.S. Citizen Only,,minimum GPA: 3.0/4.0
strack158s1,Accept,Nitin,Daphalapurkar,nitin@lanl.gov,Los Alamos National Laboratory,Mechanics of granular materials--coupled continuum-mesoscale modeling,Solid Mechanics; Materials science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Machine Learning and AI","Mesoscale simulations implementing inelastic deformation of granular matter for accurate predictions of a moving shock. Unlike solids and fluids, the dynamic response of granular materials has the additional complexity of being dominated by the presence of irregular spaces between the grains. Under dynamic deformation, a grain squashes from impact, crushes, and squeezes through available pore spaces. The statistical nature of the complex shock response arising from tens of thousands of grains may result in a different compaction state than what was intended during its performance. The solution to this problem is at the intersection of geoscience and computational solid mechanics, which strives to develop a physically based, mesoscale-consistent continuum description for granular materials.","Numerical simulations using solid mechanics methods, e.g. Finite element methods; discrete element methods, material point method. Multiscale coupling of physics, e.g. definition of stress tensor, temperature and other such variables; Machine Learning approach for predicting granular materials under pressure and shear loading.",,Minimum GPA (specify what GPA in comments below); In-Person Only; U.S. Citizen Only; Permanent Resident OK; International OK,,
strack160s1,Pre-Match+Workshop,Mitchell,Wood,mitwood@sandia.gov,Sandia National Laboratories,Data-driven Models for Material Science and Beyond,Multiscale Materials Modeling,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing; Machine Learning and AI; National Security","Experiments to study materials extreme environments (temperature, pressure, strain rate) are challenging and time consumptive, therefore we turn to modeling tools to refine and predict outcomes beforehand. Ideal computational models balance absolute physical accuracy against approximate but computationally lightweight constitutive inputs. In addition, with exascale super computers arriving in the near future, it is timely to ask whether our simulation software is capable of matching this unprecedented computing capability. While many research challenges in material physics, chemistry and biology lie just out of reach on peta-scale machines due to length and time restrictions inherent to Molecular Dynamics and electronic structure methods, questions of the accuracy of our predictions will continue to linger. This is particularly true for complex alloys, composites of disparate components as well as materials in extremes of temperature, pressure and radiation exposure. Here we aim to break the normal accuracy-cost tradeoffs by using machine learned models that scale to the largest supercomputing platforms in the world.","Material Science, Physics, Chemistry, Python, High Performance Computing",,In-Person Only; U.S. Citizen Only,,
strack161s1,Accept,Warren,Davis,wldavis@sandia.gov,Sandia National Laboratories,In-Situ Machine Learning,Computer Science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","DOE research often involves discovering new, â€œinterestingâ€ events in high-fidelity physics-based HPC simulations. Standard anomaly detection algorithms are limited, requiring the capture of all the data for post processing, or requiring high-bandwidth in-situ communication. Our research focuses on creating more efficient ways to detect anomalies in-situ, thus facilitating more precisely targeted event capture.",Python programming Mathematics,,Minimum GPA (specify what GPA in comments below),,3
strack162s1,Accept,Suaznne,Parete-Koon,paretekoonst@ornl.gov,Oak Ridge National Laboratory,HPC/AI/Data Sci. Project-Based Curriculum Building,"HPC, AI, Data Science","Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","One of the best ways to learn a new skill it to teach it. This project will focus on preparing introductory High-Performance Computing (HPC) material to enable individuals to gain foundational HPC skills for advanced computing and bridge the gap between advanced HPC programs and currently available courses in Computer Science undergraduate curriculum. The goal is to provide clear and easily accessible tutorials that will educate newcomers on the technical aspects of HPC. These tutorials will include definitions of commonly used HPC terms, concepts, applications, and hands-on programming exercises. This project will contribute to a future diverse HPC workforce by making HPC and Data Science more accessible, not just technically but addressing cultural and economic barriers to reach out to underrepresented populations. Students will first apply the core computing skills to smaller-scale datasets and codes and develop more advanced skills, such as parallelizing their code and handlining larger datasets. They will then appreciate the value of parallel computing and how one can harness the analytics capabilities of HPC to solve complex computational problems with high societal impact. We believe this project aligns with the goals of the SRP program in building the future workforce for HPC, AI, and Data science.",Passion to learn,"This team of mentors includes Suzanne Parete-Koon, John Holman (holmenjk@ornl.gov), Togo Odbadrakh (odbadrakhtt@ornl.gov , and William Godoy (godoywf@ornl.gov)- We are open to mentoring 1 ECP Intro the HPC Bootcamp student and 1 Student and Faculty team. Please no more than that! We are all running a Hackathon at OLCF during the matching workshop and will need to rotate through the workshop. Please invite all of us. Is there a place that I can submit everyone else's bios and pictures? I think having my teammates bios will help students choose us. We are a village of HPC skills and mentorship.",other,"Since I don't know our funding source, I can guarantee that we can host International- but not opposed in principle.",
strack163s1,Accept,Megan,McCarthy,megmcca@sandia.gov,Sandia National Laboratories,Accelerating machine-learned interatomic potential development for nanoscale materials science,Computational materials science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.)","Our group uses a combination of atomistic multiscale simulation methods, paired with machine learning techniques, to create powerful near-quantum-accurate models for atomic interactions. These models are called machine-learned interatomic potentials and are used in molecular dynamics (MD) simulations, which simulate a material atom-by-atom. MD is widely used to both understand nanoscale behavior in well-known engineering materials such as iron and silicon and also to discover and characterize new materials such as high entropy alloys. In our group, we look specifically at materials used extreme environments, such as those found inside nuclear fusion and fission reactors, and simulate them at very large scales on high-performance computing systems (HPC, also sometimes called supercomputers). Creating new interatomic potentials for MD is very challenging and is thus currently a huge bottleneck in the world of nanoscale materials modeling. The intern's project would involve working with an interdisciplinary team of scientists and other interns on tackling the science of interatomic potential development itself, with an aim to speed up production of these machine-learned potentials and thus accelerate materials science discovery. The specific project would be adjusted to the student's background and interests.","Materials science and engineering (all specialties welcome), coding / programming, data science",Some coding background (1 or 2 standard undergrad classes) is ideal but is not strictly required. No background in machine learning techniques is required.,U.S. Citizen Only; Permanent Resident OK,,
strack164s1,Accept,Mark,Miller,miller86@llnl.gov,Lawrence Livermore National Laboratory,Visualization and Data Analysis,Visualization and Data Analysis,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing",We have various projects related to scientific visualization and data analysis. We also have projects aimed at investigating and tuning performance of visualization and analysis workflows. We can work together to help refine specific goals for a summer internship.,"An interest in working on high performance computing projects involving visualizing and analyzing data. Some experience with writing and debugging software (Java,C,C++,Fortran,Python,Basic,etc.) would be useful.","In-person, remote and hybrid possible. In-person preferred but non-essential.",other,None,International likely ok only via subcontract to SHI
strack169s1,Accept,Patricia,Grubel,pagrubel@lanl.gov,Los Alamos National Laboratory,Software development,High Performance Computing,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","We are looking for students interested in applying their computer science skills while learning about real world applications. The work will be on helping develop portions of a workflow orchestration system that interfaces with the user, application and the underlying HPC systems.","Python, some database experience, Object Oriented Programming would be desirable. Linux bash scripting. Interest in containerization of applications (be willing to learn), cloud computing (optional) or work on front end gui may also be desirable.",,Minimum GPA (specify what GPA in comments below); In-Person Only; U.S. Citizen Only; Permanent Resident OK,,"3.0, some restrictions on international students"
strack170s1,Accept,Philipp,Edelmann,pedelmann@lanl.gov,Los Alamos National Laboratory,A modern approach to implicit fluid dynamics,Computational Fluid Dynamics,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","Computational fluid dynamics (CFD) is used every day in many fields of science from biology to astrophysics. Yet simulating slow flows (compared to the speed of sound) is a hard problem both in terms of numerical accuracy and in terms of efficiency, in particular when explicit time stepping is used. Recent work from stellar astrophysics has shown that it is feasible to run three-dimensional (3D) CFD simulations with fully implicit time stepping. This is significantly more complex and involves the use of linear and nonlinear iterative solvers. The trends of the past years and especially the new supercomputers of the exascale era have shown that support for GPUs is essential. At the same time traditional methods of parallelization using the Message Passing Interface (MPI) have trouble coping with heterogeneous (from different physics modules) workloads on large systems, which is why task-based approaches to parallelization have gained popularity. LANL developed a modern C++ framework FleCSI to help writing multiphysics codes while providing abstractions for details of the task runtime and GPU vendor library. We propose to implement a prototype 3D CFD code using FleCSI and our own solvers library FleCSolve and test it on LANL's new supercomputer Vendado.","The most important thing for this project is an interest and enjoyment of programming and numerical simulations. This is an exploratory project that can be focused on different aspects of computer science, applied math, or physics depending on the interests of the student. A general background in programming is required, but other skills can be picked up during the project as needed. Other useful (but not required) skills are: modern C++, (non)linear iterative solvers, computational fluid dynamics methods, high-performance computing",,Permanent Resident OK; International OK,,
strack171s1,Accept,Ember,Sikorski,elsikor@sandia.gov,Sandia National Laboratories,Material Models for Fusion Energy,Atomistic Materials Modeling,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.)","Achieving nuclear fusion power on earth requires incredibly durable materials capable of housing a tiny sun. We can use quantum materials modeling to make predictions about the behavior of prospective fusion materials, without the need for experiments. However, quantum models are very computationally expensive. Machine learning allows us to bypass much of the expense and enable larger scale modeling with quantum accuracy. With these models we can study thermomechanical properties and/or plasma interactions in fusion materials. Now, we can design fusion materials at the nanoscale for better performance. Project ideas include classical modeling of the fusion material, quantum modeling for the machine learning training set, or optimizing the code to better find the machine learning hyper-parameters.","Looking for students interested in applied research for nuclear energy, aerospace, or other applications with extreme environments.","If interested, please contact Meg McCarthy (SNL) as I will be unavailable during the matching workshop.",In-Person Only; U.S. Citizen Only,,
strack172s1,Accept,Roel,Van Beeumen,rvanbeeumen@lbl.gov,Lawrence Berkeley National Laboratory,Randomized solvers for tensor eigenvalue problems,Numerical Linear Algebra,High-Performance Computing Quantum Computing and Information Science,"Eigenvalue computations are at the core of simulations in many applications, including quantum physics, material science, and electronic structure computations. On the other hand, randomized algorithms are currently gaining more attention because of their potential for both reducing computational complexity and data movement on the emerging heterogeneous computing systems. Project ideas could include comparing and testing different randomization schemes, incorporating mixed-precision, improving (CPU/GPU) performance, etc. Current and near-term quantum computers, also known as noisy intermediate-scale quantum (NISQ) computers, are characterized by low qubit counts, short qubit decoherence times, and high gate error rates. On the other hand, rapid progress in both quantum hardware and software results in continuous simulation needs of novel/modified quantum algorithms. The QCLAB++ simulation software package, developed at LBNL, is an object-oriented and fully templated C++ package for creating and representing quantum circuits. QCLAB++ can be used for rapid prototyping and testing of quantum algorithms, and allows for fast algorithm development and discovery. Project ideas could include improving (CPU/GPU) performance, adding different noise models, and expanding the quantum algorithmsâ€™ base.","Specific background depends on the specific project focus, but generally: applied mathematics, numerical algorithm design, software development (C++, Matlab), etc.",,In-Person Only,,
strack175s1,Accept,Mark,Paris,mparis@lanl.gov,Los Alamos National Laboratory,Advanced computational methods for applied nuclear reaction theory,Theoretical and computational nuclear physics,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","The Theoretical Division at Los Alamos National Laboratory brings a diverse array of expertise to understanding nuclear fusion and fission reactions -- the fundamental, underlying component of systems driven by nuclear fuels. Exciting, recent developments in confined fusion systems at facilities around the world demonstrate that terrestrial nuclear fusion is attainable, reinforcing the importance of understanding the basic nuclear reactions governing these systems. In addition to their relevance in the production of terrestrial energy reactors, nuclear reactions are foundational elements in understanding energy production and structure of stars, the ``Big Bang'' of the early universe, and the extreme phenomena of explosive supernovae and their production of neutron stars and black holes. Our mentoring team (Hlophe, Lovell &amp; Paris) leverages the diverse personal backgrounds and expertise of its members to provide effective mentorship for students from a wide variety of socioeconomic environments and levels of academic preparation. Theoretical Division scientists seek to broaden and deepen our research portfolio in theoretical and computational nuclear physics. We employ standard, few- and many-body techniques in combination with optimization methods, to more efficiently investigate reactions of importance for these sorts of applications in fundamental science, nuclear safety and security, and nuclear energy.","Interest in, excitement about, and experience in solving physics problems with computers are all welcome.","We are planning to have a co-mentorship model with three T-2 staff members (Amy Lovell, Linda Hlophe, and Mark Paris). Amy is an early career scientist, with T-2 since 2018. Linda Hlophe is taking his first visiting assistant professorship (with MSU) in T-2 since August of this year. Mark Paris has been in T-2 since 2012.",Permanent Resident OK; International OK,,
strack176s1,Accept,Erika,Ye,erikaye@lbl.gov,Lawrence Berkeley National Laboratory,Quantum-inspired methods for solving PDEs,High-dimensional PDEs,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Quantum Computing and Information Science","Partial differential equations are used across many scientific fields to describe physical phenomena. However, nonlinear PDEs can be difficult to understand analytically and also difficult to simulate numerically. In particular, numerical simulation methods suffer from the curse of dimensionality, meaning that the number of degrees of freedom increase exponentially with problem dimension. Quantum-inspired algorithms offer a way to systemically reduce the degrees of freedom, potentially allowing for (sub)exponential reduction in computational cost. These algorithms are gaining in popularity and have been used to solve Navier-Stokes and the Vlasov equation with surprising success. This project can take many directions, such as extending these methods to different PDEs or considering problems with complex geometries.",Coding experience Linear algebra Basic quantum mechanics Numerical simulation background,I may have to missed parts of the matching workshop,Minimum GPA (specify what GPA in comments below); Permanent Resident OK; International OK,,3.0/4.0
strack177s1,Accept,Bert,Debusschere,bjdebus@sandia.gov,Sandia National Laboratories,Uncertainty Quantification in Computational Simulations,Computational Science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","Uncertainty Quantification (UQ) is the assessment of the confidence in we can have in computational results based on the uncertainty in the model and its input parameters. It also involves the calibration of models based on information in available data sets. Our workgroup focuses on the development and implementation of UQ methods in open-source software, as well as the application of these methods to relevant problems in computational sciences. We are looking for 1 â€“ 2 interns to help with the software engineering of our Python open-source UQ tools, as well as the application of these tools to studies in nuclear fusion energy.","The skills we look for will depend on the project focus, but generally we are interested in any of the following: an inquisitive mind, software development (Python), applied mathematics, statistics, Bayesian methods, github, etc.",The topic area described here acts as an umbrella for many possible internship projects. We will be happy to come up with specific projects that fit your skills and interests.,International OK,,
strack178s1,Accept,David,Williams-Young,dbwy@lbl.gov,Lawrence Berkeley National Laboratory,Enabling the Next Generation Quantum Simulations with Modern High-Performance Computing,Computational Physics and Numerical Linear Algebra,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing","In this research, we will develop novel methods for the efficient solution of the quantum many-body problem by leveraging the latest advances in modern high-performance computing and sparse numerical linear algebra. Key applications of this work will be accelerating scientific simulations in computational chemistry, nuclear physics, high-energy physics and quantum information science, each of which hold significant potential to advance our understanding of the universe and the fundamental physical laws that govern it. Researchers will gain hands-on experience in these and related fields and work in a highly-collaborative environment, on-site at Lawrence Berkeley National Lab.","Programming experience and a background in quantum mechanics, computational physics and/or linear algebra are highly desirable, but not required. Experience programing GPUs and working with distributed computing (MPI) would increase the scope of potential projects.",,In-Person Only; Permanent Resident OK; International OK,,
strack179s1,Under Evaluation,Li,Tang,ltang@lanl.gov,Los Alamos National Laboratory,High-Performance Computing with Python,High-Performance Computing,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","LANL's Venado, the first NVIDIA Grace-Hopper supercomputer in the US, will be ready in early 2024. NVIDIA's Grace-Hopper integrates its latest Grace CPU and H100 GPU on the same chip for faster CPU-GPU data movement, and Venado provides an exotic liquid cooling system for maximized system performance and reliability. To run large-scale simulations using GPUs on Venado, conventional HPC programming solutions include the mix of CUDA (GPU programming) and MPI (node communication), and the Kokkos ecosystem. However, these solutions usually require significant programming (e.g., C++ and MPI) and hardware expertise (e.g., GPU architecture). To strengthen LANL's HPC productivity on the rapidly evolving heterogeneous supercomputers with GPUs, we will evaluate an innovative HPC programming paradigm, programming physics simulations using NumPy, by accelerating LANL's physics simulations on hundreds of Venado GPU nodes.","Python, NumPy",,In-Person Only; Permanent Resident OK; International OK,,
strack180s1,Accept,Ken,Raffenetti,raffenet@anl.gov,Argonne National Laboratory,MPICH: A High Performance and Widely Portable Implementation of the Message Passing Interface (MPI) Standard,Parallel Programming Models and Runtime,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing; Machine Learning and AI","MPICH is a widely used, open-Â­source implementation of the MPI message passing standard. It has been ported to many platforms and used by several vendors and research groups as the basis for their own MPI implementations. We are looking for collaboration at all levels of the software stack. Including, but not limited to, low-level networking libraries, shared memory, collective algorithms, performance testing/tuning, CI/CD, documentation, and more!",C Python Shell Networking Linux/Unix,,In-Person Only,,
strack181s1,Accept,Samuel,Reeve,reevest@ornl.gov,Oak Ridge National Laboratory,Particle-based performance comparisons with Cabana,Materials science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing","Particles (just points in a box) can be used to simulate a huge range of different processes and phenomena, from atoms to galaxies. Because there are so many similarities in writing simulation codes, improving the computational performance of those codes, and making sure the physics is correct (even with very different physics), we created a library to manage particles called Cabana. Cabana is made for high-performance computing (HPC) particle codes on all the hardware architectures (GPUs, etc.) across Department of Energy (using the Kokkos and MPI libraries). This project would focus on comparing the performance of recent Cabana applications on HPC platforms (or even potentially building a new Cabana-based code for a method of interest). Recent codes have focused heat transport, fracture and failure in materials, plasma physics, cosmology, and atomic behavior.",Any programming (especially C++) experience would be highly useful; interest in GPU computing (especially Kokkos); experience or interest in materials (from any background or domain science),I started at a national lab as a summer undergraduate student and I love continuing to work with students as a staff member!,Permanent Resident OK; International OK,,
strack182s1,Pre-Match+Workshop,Damian,Rouson,rouson@lbl.gov,Lawrence Berkeley National Laboratory,Deep learning for climate simulation,Artificial intelligence,High-Performance Computing; Machine Learning and AI,The goal of this project is to use language-based parallel and GPU programming to accelerate neural-network training and large-batch inference in the context of developing a proxy application for the cloud microphysics component of a climate model.,Programming in modern Fortran. Deep learning. Parallel or GPU programming.,,U.S. Citizen Only; Permanent Resident OK; International OK; other,in-person or hybrid work preferred,
strack183s1,Pre-Match+Workshop,Paul H.,Hargrove,phhargrove@lbl.gov,Lawrence Berkeley National Laboratory,UPC++/GASNet for lightweight communication and global address space support,"programming models, languages and libraries for high-performance computing","Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","Scientists use supercomputers to tackle some of the most challenging problems, ranging from biomedical simulations of COVID-19 spread in human lungs, to performing metagenome assembly based on environmental samples as a way to study climate change. The UPC++ library is used in both of these applications. We seek application and library developers to partner with our team, which develops parallel programming models such as UPC++ and GASNet-EX. These enable computational scientists across a broad range of disciplines to write high-performing, maintainable, parallel software for large-scale systems. If you are writing a parallel scientific application in C++, we would be glad to explore with you whether UPC++ could help with some of the more challenging aspects of your problem. UPC++ excels at handling applications and libraries that have challenging load balancing issues and irregular communication patterns. If your parallel program currently runs only in shared memory and you are interested in scaling up to larger, distributed-memory systems, we can help determine a path toward scalable performance on larger systems using UPC++.",Parallel programming in C++,"Berkeley has beautiful weather, scenic views and surfing nearby. Additionally our group has been geographically distributed since before it was cool so we know how to collaborate remotely and productively. So, regardless of whether you elect to relocate to Berkeley or not, you'll have a great experience.",Permanent Resident OK; International OK,,
strack184s1,Accept,Dan,Gunter,dkgunter@lbl.gov,Lawrence Berkeley National Laboratory,Scientific Software,Computer Science,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); Machine Learning and AI","Diverse opportunities in the areas of software engineering, UX design research and implementation, and Python programming for Chemical and Environmental Engineering applications involving surrogate modeling and equation-oriented optimization application frameworks.",Python programming or UI/UX knowledge or interest or software or release engineering or optimization applications; also any interest in documentation or Jupyter Notebook enhancements,,International OK,,
strack185s1,Pre-Match+Workshop,Kevin A.,Brown,kabrown@anl.gov,Argonne National Laboratory,Investigating and Optimizing Supercomputer Network Performance,Computer Science,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","Supercomputers use the fastest network and storage technologies to help solve big problems in artificial intelligence (AI) and climate science among other fields. However, properly configuring supercomputers requires carefully tuning the hardware and software to meet the performance needs of the work to be done. Projects ideas include creating new software to measure performance on supercomputers, analyzing and optimizing the performance of climate science and AI applications, investigating and simulating new supercomputer designs, and improving discrete event simulations.","Specific background depends on the specific project focus. Generally, experience with C or Python may help along with an interest in problem solving","This is a placeholder for a range of project ideas in the area of performance analysis, network simulations, and supercomputer designs. We can come up with specific projects based on your skills and interests.",Permanent Resident OK; International OK,,
strack186s1,Accept,Silvio,Rizzi,srizzi@anl.gov,Argonne National Laboratory,Enabling Scientific Discovery With Supercomputers,Computer Science,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization)","As the first exascale supercomputers come online, scientists are confronted with formidable challenges in analyzing the outcomes of large-scale numerical simulations. I am actively seeking enthusiastic students and faculty members who share a deep interest in exploring the intricacies of supercomputer simulations. Together, we will leverage leading-edge technologies for scientific visualization and analysis to facilitate scientific discoveries.","Experience in programming languages, such as C, C++, or Python. Experience in Computer Graphics and/or Machine Learning Frameworks is a plus.",,other,would prefer no restrictions,
strack187s1,Accept,Thomas,Smith,tmsmith@sandia.gov,Sandia National Laboratories,High-Fidelity Computational Electromagnetics and Plasma Modeling,Computational EM Plasma Science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing; Machine Learning and AI","Computational electromagnetics and plasma physics (CEMP) are two areas of computational science that combine classical physics with high performance computing (HPC) to solve important national level problems. CEMP is a frontier in computational science requiring a great deal of discovery. The push to deliver fusion energy is growing, and at the national laboratories, universities and private sector, CEMP is playing an increasingly important role and will continue to do so. The project is centered around several fundamental concerns in CEMP. The first concern relates to verification of existing CEMP codes and asks the question ""Is the code correct?"" We use various numerical analysis techniques to answer this question including: developing analytic model problems from classic solutions, and constructing surrogate models that bridge gaps between theory and CEMP codes. We then run the CEMP on HPC systems to establish code credibility. The second concern relates to validation of CEMP codes and asks the question ""Is our model correct?"" To answer this question, we run sub- and system level simulations using HPC and use available data to determine how our CEMP solutions match up and address any gaps found in our modeling.","Training in mathematical physics and/or engineering (e.g., calculus, electromagnetics, fluid dynamics, computational fluid dynamics, plasma physics), A familiarity with programming in python and/or C++ is desired but not necessary, A familiarity with the Linux operating system",No formal training in plasma physics is necessary Interest in discovering how things work and how to solve difficult problems on a computer is important,Minimum GPA (specify what GPA in comments below); U.S. Citizen Only,,"Intern will be required to gain access to our secure network, minimum GPA: 3.0"
strack188s1,Accept,Meifeng,Lin,mlin@bnl.gov,Brookhaven National Laboratory,Exascale Computing for High Energy Physics - Addressing the Portability Challenge,"High Performance Computing, Physics","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","Computing plays an essential role in High Energy Physics (HEP) research. From the ab initio theory calculations using numerical techniques such as lattice QCD, to detector simulations and data analysis for the particle physics experiments such as those at the Large Hadron Collider, the computational demand has been growing rapidly due to the increased, exabyte-scale, data volume from the experiments and enhanced precision needed to probe new physics beyond the Standard Model. The exascale massively parallel supercomputers hold the promise to provide orders of magnitude more computing power to support this fundamental physics research. However, the diverse heterogenous architectures in the exascale systems present many challenges to the scientific software developers. This SRP project will develop and benchmark mini code examples based on the use cases we have extracted as part of the High Energy Physics Center for Computational Excellence project (www.anl.gov/hep-cce). The goal is to use these ""mini-apps"" to track the progress of the software stack to support performance portability, the notion that a single source code can run efficiently on multiple architectures, and the ways to address it. The participants will be exposed to both the physics motifs and computational challenges in high energy physics.",Computer Science C++ Programming High Performance Computing Physics,The project can be tailored depending on the participant's experience and interests. And we fully expect the SRP participants to be actively engaged with the HEP-CCE team.,Permanent Resident OK; International OK,,
strack189s1,Pre-Match+Workshop,Arvind,Mohan,arvindm@lanl.gov,Los Alamos National Laboratory,Machine Learning for Surrogate Modeling in Earth Sciences,Machine Learning and Physical Sciences,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Machine Learning and AI","Climate change is already causing more frequent and severe extreme weather events, such as hurricanes, floods, droughts, and wildfires. These events can have devastating impacts on communities, livelihoods, and infrastructure. Machine learning has become an attractive alternative to developing fast surrogate models of these phenomena. ML surrogate models can help decision-makers better understand and respond to these events and mitigate their consequences. To be valid, these models need to a) Predict the likelihood and severity of future climate disasters and b) Assess the vulnerability of communities and infrastructure to climate disasters. c) Be robust and reliable for trustworthiness. However, there are many fundamental challenges to making ML models helpful in this area, and the summer project will focus on developing methods to address this issue. The topic is open-ended, and the student will have the opportunity to pick a specific direction of their liking.",1) Experience and knowledge of fluid mechanics and turbulence. 2) Good understanding of partial and ordinary differential equations 3) Experience with neural networks is desired.,,In-Person Only; Permanent Resident OK; International OK,,
strack191s1,Pre-Match+Workshop,Rafael,Zamora-Resendiz,rzamoraresendiz@lbl.gov,Lawrence Berkeley National Laboratory,Scaling Protein Structure Machine Learning Applications Using HPC,Computational Biology,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing; Machine Learning and AI","Graph convolution and self-attention deep learning models have grown in popularity in the domain of proteomics. These machine learning approaches have been used in several proteomic problems including protein classification and protein-ligand binding affinity prediction. Searching for ligands and poses with high binding affinity have immediate applications in drug design and drug repurposing. While many DL tools have been used to model the site interaction at the residue level, atom-level models would effectively increase the resolution of interactions. Even so, scaling to systems with &gt;1,000 atoms is a non-trivial task which requires HPC. Recently, much work has gone into scaling training of language model architecture like GPT and BERT using HPC. The proposed project will explore utilizing language model architectures to scale the modeling of protein structures in the context of protein-ligand binding. Model parallelism will be used to enable modeling of proteins at the atomic level. After training on large datasets of binding site interactions, these models will be tested against out-of-training examples to observe its utility in imputing binding affinities across diverse protein and ligand databases. Methods for interpreting model parameters will be developed to provide biological meaningful insights that can help in drug design and repurposing.","Relevant Skills, Background or Interests: Good understanding of programming and machine learning. Some understanding of parallel programming and working with high performance computing systems. Interested in the application of computational skills to the domain of biology and medicine. A deep desire towards improving their skills of writing well-documented and reusable scientific code.",,U.S. Citizen Only,,
strack192s1,Accept,Patricia,Gonzalez-Guerrero,lg4er@lbl.gov,Lawrence Berkeley National Laboratory,Beyond moore computer architectures,Computer architecture,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing; Machine Learning and AI","Project 1: FPGA based architecture exploration. We use FPGAs to evaluate architectural features that could improve performance in scientific computing. We then can take the FPGA tested architecture and generate ASICs. Project2: We propose to explore temporal computing, stochastic computing, or deterministic streams for more efficient computing. Project3: We propose to explore superconducting computing for low power high performance computing.",One of below: circuit design Computer architecture Signal processing,,other,none,
strack193s1,Pre-Match+Workshop,Victor,Mateevitsi,vmateevitsi@anl.gov,Argonne National Laboratory,Digital Twins,Digital Twins,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing","Robotics is changing the way new technologies are manufactured, researched, and developed. Robots can operate at high speed, with infinite precision, and without fatigue. Thus, they are ideal for repetitive work in hazardous environments like nuclear reactors. However, designing and evaluating such systems is extremely hard. A digital twin is a digital replica of the physical environment, the mechanical components, and the entire research/production workflow. You can think of a digital twin as a digital reconstruction of the lab, including all robotic arms, sensors, desks, laboratory components, tests, and how they interact with each other. Our research interests include exploring how digital twins can help diagnose operational anomalies, understand system health, and improve overall system efficiency in Virtual Reality. Imagine a nuclear physicist wearing a Virtual Reality headset and safely assembling a novel nuclear reactor inside a Virtual Environment. They can now run tests and simulations, gather sensor data, and improve their design. Once they are satisfied with the results, they can request the physical replica of the robotic laboratory to assemble the reactor and perform the pre-programmed tests.",Augmented/Virtual Reality Robotics,,In-Person Only,,
strack194s1,Pre-Match+Workshop,Johannes,Blaschke,jpblaschke@lbl.gov,Lawrence Berkeley National Laboratory,Workflow enablement on HPC,Computing,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing","This project explores various workflow technologies, and how they might be used on a Supercomputer at NERSC. Future Supercomputers will offer a broad range of capabilities at NERSC Examples relevant to scientific workflows on the include: simulation performance, data movement and management, application of machine learning, organization and composition of complex computational tasks, interaction with external resources beyond the NERSC data center (e.g. edge, cloud, and cross-facility). We will be working on developing benchmarks and test cases for a range of workflow technologies, depending on the applicant's field of study and personal interest. Some example focus areas are: 1. Exploring use cases for workflow managers, eg: Fireworks, Snakemake, Parsl, etc 2. Exploring non-MPI communication libraries, eg. Distributed.jl, Legion, etc 3. Exploring new storage technologies and tools","Experience with: 1. Linux / Git 2. Compiling software (with Make and CMake) and managing shell environments 3. Some basic programming skills in C, Rust, Julia, and/or Python are a must. Depending on the focus area, applicants should have a strong interest in scientific software workflows for HPC systems.",,Permanent Resident OK; International OK,,
strack195s1,Accept,Robert,Jacob,jacob@anl.gov,Argonne National Laboratory,Global Climate Modeling,"Climate, HPC","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing",My group is part of a multi-lab collaboration to develop a global climate model that runs well on DOE's most advanced computing facilities. We are involved in all aspects of the software development of the model and its performance on heterogeneous architectures. We also develop python tools for analyzing the data. Join our group if you are interested in any of these topics.,"Any of: high performance computing, research software engineering, data analysis, performance engineering.",,Permanent Resident OK; International OK,,
strack196s1,Accept,Justin,Wozniak,woz@anl.gov,Argonne National Laboratory,HPC Workflow Performance,"HPC, Workflows","Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software)","High Performance Computing (HPC) workflows combine physical simulations with machine learning and other analysis to address scientific problems in modeling the spread of COVID-19, designing precision cancer treatments, and other areas. The Swift/T workflow language is designed to run on the largest available supercomputers and integrate complex, parallel software packages under the control of a high-level dataflow language. Optimizing and debugging such workflows is difficult. There are range of tools developed by the computer science community to analyze the performance and correctness of such software systems. In this project, the team will apply one or more such systems to Swift/T workflows in a scientific application area. The team will run Swift/T in a simulated environment using a distributed computing simulator such as SimGrid/SMPI to evaluate correctness and performance. Technical details: Swift/T is a parallel programming language that uses a Java-based programming language translator to generate code for a C and MPI -based runtime. Swift/T can distributed and run user scientific codes written in any language, but Python and R modules are a popular choice. The system is an excellent environment in which to learn about programming language design/implementation, distributed computing, and systems software in general.",C language and Linux environment.,,Permanent Resident OK; International OK,,
strack197s1,Accept,Natalie,Isenberg,nisenberg@bnl.gov,Brookhaven National Laboratory,Modeling and Control of Particle Accelerator Beams using Bayesian Neural Networks,Surrogate modeling/AI/ML,Machine Learning and AI,"Particle accelerators are a central tool for scientific discovery, from unraveling the governing laws of fundamental particles, to understanding the universe's origins. Brookhaven National Laboratory is home to the Relativistic Heavy Ion Collider (RHIC) and the future Electron Ion Collider (EIC), two world-leading particle accelerators. At RHIC, which is the largest particle accelerator in the United States, scientists are currently studying the properties of subatomic particles. Specifically, researchers use particle collisions to recreate extreme conditions like those shortly after the Big Bang: the prevailing theory for the origins of our universe. Both cutting-edge accelerator experiments rely on complex particle beam controls (e.g., altering trajectories) to create collisions that generate useful data. Ensuring the accuracy and reliability of particle collision experiments is of critical importance and requires the development of robust beam control methodologies. In this research project, we will build a data-driven model of beam position using Bayesian neural networks (BNNs). BNNs are machine learning models that provide accurate predictions while also quantifying the error inherent in the data, ensuring robust and reliable beam position forecasts. The student intern working on this project will gain practical experience in cutting-edge machine learning topics as applied to a real high-energy physics application.","Some experience in Python or Julia preferred, but not required.",,In-Person Only; Permanent Resident OK; International OK,,
strack198s1,Accept,Neil,Getty,ngetty@anl.gov,Argonne National Laboratory,Vision Language Action Models for Robotic Surgery,"AI, Robotics, LLM, Medicine","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); Machine Learning and AI","Robotic surgery presents many motivating challenges for computing research. Robotic surgery is an interface between surgeons and patients, much in the way motor vehicles are becoming less mechanic, and more of an interface between drivers and the road. In driving, optical and radar sensors coupled with automated warning, braking, and steering actions have dramatically increased motor-vehicle safety. We expect a similar development of technologies that prevent mistakes and improve outcomes in surgery. We are partnered with the Surgical Innovation and Training Lab at the University of Illinois at Chicago, with access to a robotic surgery research kit, expert annotated surgery videos, and medical expert collaborators. Our research efforts include instrument detection and segmentation, depth estimation, instrument tracking and kinematic estimation, and skill estimation applied to robotic surgery. We are particularly interested in exploring foundation models, vision language, and vision language action models for robotic surgery tasks.","Programming (python, pytorch, cv2), data, machine learning, deep learning, biomedicine, computer vision",,Permanent Resident OK; International OK,,
strack199s1,Accept,Carlos Fernando,Gamboa,cgamboa@bnl.gov,Brookhaven National Laboratory,Data Storage for Scientific Experiments,Distributed Data Systems,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization)","One of the missions of the Scientific Data and Computing Center (SDCC) at Brookhaven National Laboratory is to provide access to storage services for a diverse range of High Energy Physics (HEP) scientific experiments, including LHC-ATLAS[1], Belle2[2], and DUNE[3]. An aggregate of 222 million files and data storage totaling 76PB is distributed and managed by independent storage instances for each Virtual Organization (VO). The underlying technology used to support this storage is dCache [4]. Currently, we are in the process of evolving and reviewing event visualization and control schemes, as well as log analysis tools, to improve analytics and monitoring and ensure the high availability of this storage service. Additionally, we are interested in identifying state-of-the-art tools and techniques that allow us to anticipate or predict inefficient storage resource access or real time system component failures. Ultimately, these improvements will have a significant impact on advancing scientific discovery while optimizing storage resources. [1] https://home.cern/science/experiments/atlas [2] https://www.belle2.org [3] https://www.dunescience.org [4] https://www.dcache.org",Knowledge of: -Scripting languages like Python -Programing languages like Java -Operational Systems like Linux -Databases (Relational or non) -Document databases like Elasticserach -Artificial Intelligence techniques,,U.S. Citizen Only; Permanent Resident OK,,
strack200s1,Accept,Allison,Aiken,aikenac@lanl.gov,Los Alamos National Laboratory,Aerosol Science,Earth and Environmental Science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); Machine Learning and AI","Aerosols are small particles (&lt;~10 microns in diameter) that are suspended in a gas, like the Earthâ€™s atmosphere. These particles are central to understanding the water cycle and transport of nutrients within the Earth System. A complete understanding cannot be provided without ground-based and vertically resolved observations, particularly for aerosol-cloud interactions. The goal of this study is to analyze diverse and large datasets collected by the U.S. DOE Atmospheric Radiation Measurement (ARM) mobile facility during campaigns that we have deployed instrumentation to and are actively involved in with funding from the Atmospheric System Research (ASR). Two examples include the Surface Atmosphere Integrated Field Laboratory (SAIL) campaign in Colorado focused on aerosol impacts on mountain hydrology and the Eastern Pacific Cloud Aerosol Precipitation Experiment (EPCAPE) in California focused on aerosol-cloud interactions. Data includes but are not limited to those collected by the Aerosol Observing System (AOS) and the tethered balloon system (TBS). Specific aims are directed at answering key aerosol process science objectives related to identifying different aerosol regimes, the processes controlling their lifecycles, quantifying impacts on the radiative budget, and the sensitivity of cloud phase and precipitation to cloud condensation nuclei (CCN) and ice-nucleating particle (INP) concentrations.","Desired relevant skills include: experience and/or exposure to different computing languages, e.g., Python, Igor, R, JupyterHub, etc. as well as applying and/or developing statistical and mathematical algorithms, e.g., positive matrix factorization, K-means clustering, machine learning/AI, etc. Examples of desired interests include but are not limited to: large data science, environmental science, atmospheric science, high-time resolution measurements, chemistry and mass spectrometry, physics/optics, single-particle measurements, climate science.",,other,None known.,
strack202s1,Accept,Jay,Lofstead,jay@lofstead.org,Sandia National Laboratories,Accelerating Scientific Insights Through Advanced Data Exploration,"computer science, computational science","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing","Large scale science simulations and observations both generate enormous data volumes that must be analyzed to generate scientific insights. Sometimes, this analysis are simple min/max to support visualization. Other times, they are complex or derived quantities. For example, an air pressure gradient can show both a hurricane as well as straight-line winds. Being able to query for places where the gradient meets criteria is more general and helpful when looking for severe weather events. This work will help support managing these derived quantities as well as supporting the query operations. No prior knowledge of the mathematical operations nor special machine learning or database skills required. Some exposure to ease on the job learning is desired.","basic database, basic machine learning",,In-Person Only,,
strack203s1,Accept,Rajshree,Deshmukh,rajshreed@lbl.gov,Lawrence Berkeley National Laboratory,User experience design &amp; development for Science softwares,"User Experience Design, Web development","Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization)",The student will work on interface design and development for a particular scientific application. This opportunity will give hands on experience of ideating on complex user needs in research domain and coming up with solutions.,"Students with interests in user experience design, data visualization, web development can apply",,Permanent Resident OK; International OK,,
strack204s1,Accept,Paul,Hovland,hovland@mcs.anl.gov,Argonne National Laboratory,ML-based compression for derivative computation,Automatic differentiation,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Machine Learning and AI","Efficient computation of derivatives for both computational science and machine learning applications often relies on the so-called reverse mode of automatic differentiation (autodiff). Unfortunately, reverse-mode autodiff requires saving many intermediate states, which can lead to substantial memory or storage requirements. This project will investigate whether lossy compression techniques based on machine learning can be used to compress this data and reduce memory requirements while maintaining suitable levels of accuracy in the derivative computations.",Basic understanding of derivatives (first semester calculus) Programming in python or a related language Interest in machine learning,,Permanent Resident OK; International OK,,
strack205s1,Accept,Tim,Waters,waters@lanl.gov,Los Alamos National Laboratory,Radiation-MHD modeling of accreting black holes,Magnetohydrodynamics and radiation hydrodynamics,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing","I am interested in pursuing collaborations to work on topics in radiation-MHD modeling for black hole astrophysics. Specific projects include deriving dispersion relations for the linearized radiation-MHD equations, calculating the observables obtainable from reverberation mapping campaigns, modeling outflows in low-mass X-ray binaries, and modeling accretion flows around supermassive black holes. These projects range from semi-analytic, requiring only knowledge of basic complex function theory and python programming, to fully numerical, requiring knowledge of C++ and high-performance computing.","Python or C++ programming, courses on plasma physics or MHD, experience running hydrodynamical simulations, interest in learning high performance computing.",,In-Person Only; Permanent Resident OK,,
strack206s1,Accept,William,Godoy,godoywf@ornl.gov,Oak Ridge National Laboratory,Exploring HPC+AI workflows in Julia,High-performance computing,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing",We plan to explore the use of the Julia programming language on constructing high-performance computing (HPC) and AI workflows. We look towards the advancement of HPC software by incorporating a unifying high-level and high-performance Julia language as a scientific layer. We will explore cases on Frontier and Summit (if available).,High-performance computing Programming Data analysis AI Scientific Software,,In-Person Only; International OK,,
strack207s1,Pre-Match+Workshop,Sandeep,Madireddy,smadireddy@mcs.anl.gov,Argonne National Laboratory,Probabilistic Machine Learning for Scientific Data,Scientific Machine Learning,Machine Learning and AI,"ome unique challenges in scientific data that needs to be considered while building data-driven models are: (1) noise and uncertainty, (2) data scarcity, and (3) large feature spaces. Probabilistic models are a natural choice to address many of these challenges and provide a systematic approach to reason about the prediction uncertainty. Historically, the adoption of probabilistic modeling approaches has been limited by the scalability of the inference approaches. With the recent advances in Bayesian deep learning, approximate inference approaches and their information-theoretic connections have enabled inference on large-scale (with millions of parameters) models efficiently with modest computational requirements, obtaining state-of-the-art predictive accuracy. This project would involve building such probabilistic deep learning models for applications such as equilibrium reconstruction of plasma profiles in a magnetically confined fusion tokamak and detecting strong gravitational lenses from astronomical observations from telescopes.","Python programming, applied mathematics/statistics",,other,None,
strack208s1,Accept,Kibaek,Kim,kimk@anl.gov,Argonne National Laboratory,Federated Learning at Edge,"machine learning, edge computing",Machine Learning and AI,"Federated learning (FL) is a method where multiple devices train machine learning models locally on their own data, and then collaboratively refine a shared model by exchanging updates, without directly sharing the actual data. This project aims to address the technical challenges by developing and deploying FL models to the edge devices available through Argonne National Laboratory. Closely working with multiple scientists at the lab, the participants will develop, analyze, and test the FL model and training algorithms on high-performance computing cluster first and then on the edge devices.","Experience with python, PyTorch, machine learning.",,In-Person Only; International OK,,
strack209s1,Accept,Wibe Albert,de Jong,wadejong@lbl.gov,Lawrence Berkeley National Laboratory,"Enabling scientific discovery with HPC, AI and Quantum",AI and Quantum,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing; Machine Learning and AI; Quantum Computing and Information Science","One of the big challenges in finding new molecules that can better capture solar energy, to design molecular crystals that can better capture CO2 from the air, or materials that can convert heat into electricity is the very large search space of possibilities. We can accelerate this search with AI and machine learning methods that allow us to quickly determine if a molecule or material has the desired properties, and if it can be made, but this search can still take forever. We have been developing approaches for inverse design. Projects can involve the further development of our inverse design methods, or exploring our tools for real science questions. Quantum computing has received a lot of attention. We now have noisy quantum computers online on which we can do actual experiments. Our projects range from developing algorithms and simulation approaches that translate scientific problems we run on classical computers to problems that can be run on quantum computers...and we run them on quantum computers to get scientific results.","For AI related research, some understanding of machine learning approaches and some cursory reading of the literature with respect to inverse design for chemistry and materials. For quantum related research, a basic understanding of quantum computing would be a benefit. Reading one of the popular quantum computing books, for example the first parts of Nielsen and Chuang (Quantum Computation and Quantum Information), and doing some of the online qiskit tutorials will give you a great head start.",,In-Person Only,,
strack210s1,Accept,Cody,O'Donnell,ctodonnell@lbl.gov,Lawrence Berkeley National Laboratory,User Experience for Scientific Software Applications,UX/UI Design and Development,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization)","User experience design is frequently an afterthought for scientific software applications. Nonetheless, the actual experience a person has while using a piece of software can often make or break its success. Our team is working to make sure that every scientific team has the ability to build software applications that are intelligently designed, highly usable, and effective for its users. We are in the process of building a suite of design and software tools that break scientific processes into their most repeatable components and task flows. The goal is for these components and task flows to be the building blocks for new scientific applications. This means we are building both visual designs and actual code that teams will base their applications around. These tools have the potential to transform the way scientific teams approach the design of software and user interfaces. We are working with people from a breadth of scientific areas because these tools are meant to span the whole scientific domain. For example, we are or will be collaborating with teams in environmental science, water optimization, materials science, genomics, computer science, and physics.","Interested in: Web development, user interface design, JavaScript, React, software development, user experience research, data visualization, information architecture",,Permanent Resident OK; International OK,,
strack211s1,Accept,Meng,Meng,mengm@lanl.gov,Los Alamos National Laboratory,Mentor students,Rock mechanics,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); High-Performance Computing","My expertise covers advanced multi-physics geomechanics testing, well integrity, cement materials, poromechanical modeling, and data analysis. With these skills, I have participated in many different projects, including well integrity and stability for geothermal and CCUS, wellbore and reservoir stimulation for geothermal and unconventional, injection induced seismicity for geothermal and carbon sequestration, and data analysis for CCUS. I believe there are still lots of opportunities for Petroleum Engineering with application to the energy transition.","I hope to work with students with rock mechanics, petroleum, civil, mathematics, and mining background. If the students are interested in geothermal and carbon sequestration, we can find more topics to work on.",,Permanent Resident OK; International OK,,
strack214s1,Accept,Nirmal,Prajapati,prajapati@lanl.gov,Los Alamos National Laboratory,"Extreme scale computing through co-design of applications, algorithms, and architectures",compilers/languages/architectures/modeling,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); High-Performance Computing; Machine Learning and AI","Lead scientific simulations at extreme scale through the co-design of applications, algorithms, and architectures. Deliver solutions for multiple science, engineering, and security missions including the development of methods for large-scale high performance computing applications. Work on a wide array of scientific problems ranging from weapons research and algorithmic co-design to compiler technologies and machine learning that enable scientists to concentrate on science rather than programming. Develop scientific software frameworks that are performance portable for different HPC architectures. Implement high-performing versions for important applications at the lab. Profile CPU/GPU (Graphics Processing Unit) codes, identify bottlenecks, improve the performance of applications using optimization techniques. Investigate and prototype performance improvements along with verification of outputs. Run codes on LANLâ€™s cluster and/or non-LANL supercomputers. Write codes in Python, C/C++ and CUDA (language for programming GPUs) and/or HIP (language for AMD GPU architectures). Use profiling tools, applied machine learning techniques and code transformations to improve the performance of applications.","Some background or coursework in applied machine learning, code transformations, compilers, programming languages, performance optimization, parallel processing is preferred. Desired skills include LLMs, CNN, Python, C/C++, CUDA, OpenMP, etc. but not required.",,Minimum GPA (specify what GPA in comments below); International OK,,3.0 or above
strack215s1,Accept,Oliver,Ruebel,oruebel@lbl.gov,Lawrence Berkeley National Laboratory,AI for protein structure prediction and analysis,Computational biosciences and machine learning,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); Machine Learning and AI",The goal of this project is to enhance the analysis and prediction of protein structures from sequence and experimental data.,Machine learning Protein structure analysis,,In-Person Only,,
strack220s1,Accept,Talita,Perciano,tperciano@lbl.gov,Lawrence Berkeley National Laboratory,Probabilistic Graphical Deep Learning Field,"Quantum computing, computer science","Machine Learning and AI, Data Science (i.e., data analytics, data management &amp; storage systems, visualization); Machine Learning and AI; Quantum Computing and Information Science","A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statisticsâ€”particularly Bayesian statisticsâ€”and machine learning. One of the main advantages about this techniques is its ability to use prior information (physical constraints) related to the data. This becomes very important when analyzing scientific data. This project aims to develop efficient PGM-based algorithms to tackle problems such as image segmentation, image denoising, feature tracking, data reduction, data fusion, etc. We use mainly Markov Random Fields and Conditional Random Fields, and some of our approaches combine these methods with deep learning algorithms. Quantum computing is a research area that has received a great amount of attention in the last few years. In this project, we aim to take advantage of quantum computing theory to develop quantum data analysis and quantum machine learning tools suitable to the analysis of scientific data. This includes the development of new quantum circuits for quantum data representation and for analysis algorithms (feature extraction, template matching). We also aim to develop innovative quantum machine learning algorithms targeting/combining NISQ devices and HPC. We aim to develop concrete proof-of-concept tools that run on NISQ devices.","Related interests: Python and C++ programming languages, data analysis, statistics, applied mathematics, image processing, deep learning, machine learning.",,other,None,
strack221s1,Accept,Anna,Giannakou,agiannakou@lbl.gov,Lawrence Berkeley National Laboratory,Informing the next generation of HPC networking using intelligent data analytics,"computer science, networking, machine learning","Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","Scientific discovery depends on analyzing large amounts of experimental data that are produced from different instruments scattered around the world. In the next 10 years the amount of experimental data produced is expected to increase by at least two orders of magnitude. In order to be analyzed, data needs to be transferred fast and reliably to HPC centers introducing network performance as a critical factor for success. In this project we will use data analysis combined with machine learning to detect data movement performance patterns that will inform the design of dynamic HPC networks. We will use scientific network traffic from NERSC, one of the leading HPC facilities in the world. In this project we are looking for someone to help us understand information from different sources and ultimately unveil the key to faster, next generation HPC networks.",coding data analysis systems communication presentation,,other,none,
strack222s1,Pre-Match+Workshop,John,Wu,kwu@lbl.gov,Lawrence Berkeley National Laboratory,Effective Data Management for Large Scientific Workflows,Data Management,"Data Science (i.e., data analytics, data management &amp; storage systems, visualization); Machine Learning and AI","The Scientific Data Management (SDM) research group is broadly interested in enabling and accelerating scientific discoveries through effective data management and analysis tools and libraries. The SDM groupâ€™s research and development efforts focus on (1) scalable storage and I/O strategies, (2) autonomous data management infrastructure, (3) data life-cycle management, and (4) workflow optimization and automation. Our group actively works with data generation and analysis workflows to reduce the complexity of large scientific analyses, including complex real-time workflows that could drive the next generation of scientific user facilities. Members of the SDM group work closely with application scientists throughout the DOE community, academic and industry researchers around the world. The group has a strong history of publications and contributes to many widely used software systems. We have strong contributions to well-known I/O libraries including HDF5 and ADIOS; and are the primary developers of FastBit, FasTensor, and so on.",#NAME?,,In-Person Only,,
strack227s1,Pre-Match+Workshop,Silvia,Crivelli,sncrivelli@lbl.gov,Lawrence Berkeley National Laboratory,Using AI and HPC for Healthcare and Precision Medicine,AI for science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); Machine Learning and AI","As part of our collaboration with the VA and DOE, we are currently using natural language processing to extract important life events from electronic health records that are correlated with high suicide risk. In this project, we are looking for a student intern that is interested in the cross-section of machine learning and healthcare that can help us develop a user-friendly interface for clinicians and researchers. Ideally, this UI will help accelerate better decision-making.","Knowledge of programming, frontend UI development, data visualization, problem solving",,other,none,none
strack228s1,Accept,Amal,Gueroudji,agueroudji@anl.gov,Argonne National Laboratory,Performance Analytics of Dask Workloads,Computer science,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing","Dask is a task-based distributed Python framework with the unique feature of offering distributed versions of well-known libraries like NumPy, Pandas, and Scikit-learn. It has been utilized for high-performance data analytics within high-performance computing (HPC) workflows. While it boasts a strong performance record, it currently lacks an analysis of I/O performance, which is critical for HPC workloads. In this project, the selected student will utilize Darshan to extract and analyze I/O operations in Dask. The primary objectives are to gain an understanding of I/O performance and subsequently propose solutions for its improvement.","Knowledge of distributed programming and data analytics is good, as well as curiosity to discover new topics and fields, in order to generate future research directions.",,International OK,,
strack229s1,Accept,Maria,Chan,mchan@anl.gov,Argonne National Laboratory,AIMaterials,Computational Materials Science,"Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","We use a combination of AI/ML and computational modeling to understand and design energy materials. In particular, we work on (1) predicting materials properties using a combination of first principles calculations and ML; (2) seeing materials changes in the nanoscale through interpreting x-ray and electron microscopy data; (3) using computer vision and language models to create intelligent knowledge extraction software. These approaches are applied towards energy storage, photovoltaics, or catalysis.","Proficiency in Python programming is required. Knowledge of some of the following is beneficial: batteries, solar cell, catalysis, solid state physics, first principles or atomistic simulations, machine learning/artificial intelligence, computer vision, language models.",,Permanent Resident OK,,
strack230s1,Accept,Gunther,Weber,ghweber@lbl.gov,Lawrence Berkeley National Laboratory,Visualization of Scientific Machine Learning Models,Data visualization; machine learning,"Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","Scientific Machine Learning (SciML) is a growing research area that uses innovative machine learning solutions to obtain scientific insights from large data sets. Understanding these novel tools requires analyzing high-dimensional functions that have properties very different from those seen in other, more traditional DOE applications. For example, the features of a loss as a function of neural network (NN) weights provide valuable insights into properties of architectures that affect convergence during training. Compared to high-dimensional functions in other DOE-relevant applications, such as potential energy landscapes in chemistry, ML loss landscapes have many more dimensionsâ€”millions or billions of weights compared to tens or hundreds of degrees of freedom for molecules. Initial work within the ML community on visualizing loss landscapes has shown great promise, e.g., by relating NN architecture choices to the smoothness of the function being optimized during the learning process. More sophisticated visualization techniques can provide a much deeper understanding of the properties of NNs, which is particularly important for SciML problems. The goal of this project is to develop new visualization and data analysis approaches, e.g., based on topological data analysis, to help better understand these functions and as a result SciML models.","data visualization, dimension reduction, multidimensional scaling, topological data analysis, machine learning, user interface design",,Permanent Resident OK; International OK,,
strack232s1,Under Evaluation,David I.,Santiago,david.i.santiago@lbl.gov,Lawrence Berkeley National Laboratory,Experimental Quantum Computing,Superconducting Quantum Processors,Quantum Computing and Information Science,"We run a team where we design, fabricate, deploy superconducting quantum processors. We also use our processors to run state of the art quantum algorithms and protocols.","Physics, quantum information science, strong knowledge of E&amp;M, computer programming, some knowledge of quantum mechanics",,In-Person Only,,
strack233s1,Pre-Match+Workshop,Lavanya,Ramakrishnan,lramakrishnan@lbl.gov,,Data lifecycle management,Computer science,"Computer Science (i.e., architectures, compilers/languages, networks, workflow/edge, experiment automation, containers, neuromorphic computing, programming models, operating systems, sustainable software); Data Science (i.e., data analytics, data management &amp; storage systems, visualization); High-Performance Computing; Machine Learning and AI","Automation, self-guiding and self-driving are increasingly using algorithm-driven adaptive measurements for operation of end-to-end workflow. There is a need for a number of AI techniques, workflow and data management techniques to develop the infrastructure.","Computer science background with working knowledge of one or more of Python, ML/AI, Javascript, databases",,International OK,,
strack236s1,Accept,Slaven,Peles,peless@ornl.gov,Oak Ridge National Laboratory,Complex Systems,Computational Science,High-Performance Computing,Develop scalable analysis methods for power systems that can perform exascale-level computations in support of grid planning and operation.,"C/C++, CUDA, HIP, Python, Numerical Linear Algebra, Numerical Integration, Statistical Analysis, Machine Learning, Power Systems, Power Electronics, or any combination of those skills.",,Permanent Resident OK; International OK,,
strack238s1,Accept,Mark,Taylor,mataylo@sandia.gov,Sandia National Laboratories,visualization for atmospheric simulation output,"Atmospheric science, Computational science, Applied Mathematics","Computational Science Applications (i.e., bioscience, cosmology, chemistry, environmental science, nanotechnology, climate, etc.); Data Science (i.e., data analytics, data management &amp; storage systems, visualization)","Produce visualizations from high resolution E3SM global climate simulations. Using python and matplotlib and large E3SM 3km global atmosphere data sets, Find interesting phenomena in the data sets, such as tropical cyclones, atmospheric rivers and mesoscale cloud systems. Plot various fields such as total water content, precipitation rate, cloud reflectivity, superimposed on images of the earth showing oceans and land masks. Generate high resolution publication quality images and animations suitable for use in journal articles, presentations and E3SM publicity.","Some programming experience in a high level language (python, matlab), linear algebra or numerical linear algebra, interest in linux command line work. Interest in learning python-based visualization libraries (matplotlib, holoviz)",,Permanent Resident OK,,
